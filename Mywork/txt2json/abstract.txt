
A Pixel-Level Meta-Learner
Few-shot semantic segmentation addresses the learning
task in which only few images with ground truth pixel-level
labels are available for the novel classes of interest. One
is typically required to collect a large mount of data (i.e.,
base classes) with such ground truth information, followed
by meta-learning strategies to address the above learning
task. When only image-level semantic labels can be ob-
served during both training and testing, it is considered
as an even more challenging task of weakly supervised
few-shot semantic segmentation. To address this problem,
we propose a novel meta-learning framework, which pre-
dicts pseudo pixel-level segmentation masks from a limited
amount of data and their semantic labels. More impor-
tantly, our learning scheme further exploits the produced
pixel-level information for query image inputs with segmen-
tation guarantees. Thus, our proposed learning model can
be viewed as a pixel-level meta-learner. Through exten-
sive experiments on benchmark datasets, we show that our
model achieves satisfactory performances under fully su-
pervised settings, yet performs favorably against state-of-
the-art methods under weakly supervised settings.

Addressing out-of-distribution label noise in webly-labelled data
A recurring focus of the deep learning community is to-
wards reducing the labeling effort. Data gathering and
annotation using a search engine is a simple alternative to
generating a fully human-annotated and human-gathered
dataset. Although web crawling is very time efficient, some
of the retrieved images are unavoidably noisy, i.e. incor-
rectly labeled. Designing robust algorithms for training on
noisy data gathered from the web is an important research
perspective that would render the building of datasets eas-
ier. In this paper we conduct a study to understand the type
of label noise to expect when building a dataset using a
search engine. We review the current limitations of state-
of-the-art methods for dealing with noisy labels for image
classification tasks in the case of web noise distribution. We
propose a simple solution to bridge the gap with a fully clean
dataset using Dynamic Softening of Out-of-distribution Sam-
ples (DSOS), which we design on corrupted versions of the
CIFAR-100 dataset, and compare against state-of-the-art
algorithms on the web noise perturbated MiniImageNet and
Stanford datasets and on real label noise datasets: WebVi-
sion 1.0 and Clothing1M. Our work is fully reproducible
https://git.io/JKGcj.

AE-StyleGAN: Improved Training of Style-Based Auto-Encoders
StyleGANs have shown impressive results on data gener-
ation and manipulation in recent years, thanks to its disen-
tangled style latent space. A lot of efforts have been made
in inverting a pretrained generator, where an encoder is
trained ad hoc after the generator is trained in a two-stage
fashion. In this paper, we focus on style-based generators
asking a scientiﬁc question: Does forcing such a genera-
tor to reconstruct real data lead to more disentangled la-
tent space and make the inversion process from image to
latent space easy? We describe a new methodology to train
a style-based autoencoder where the encoder and genera-
tor are optimized end-to-end. We show that our proposed
model consistently outperforms baselines in terms of image
inversion and generation quality. Supplementary, code, and
pretrained models are available on the project website1.

Morph Detection Enhanced by Structured Group Sparsity
In this paper, we consider the challenge of face morphing
attacks, which substantially undermine the integrity of face
recognition systems such as those adopted for use in border
protection agencies. Morph detection can be formulated
as extracting ﬁne-grained representations, where local dis-
criminative features are harnessed for learning a hypothe-
sis. To acquire discriminative features at different granular-
ity as well as a decoupled spectral information, we leverage
wavelet domain analysis to gain insight into the spatial-
frequency content of a morphed face. As such, instead of
using images in the RGB domain, we decompose every im-
age into its wavelet sub-bands using 2D wavelet decompo-
sition and a deep supervised feature selection scheme is em-
ployed to ﬁnd the most discriminative wavelet sub-bands of
input images. To this end, we train a Deep Neural Network
(DNN) morph detector using the decomposed wavelet sub-
bands of the morphed and bona ﬁde images. In the train-
ing phase, our structured group sparsity-constrained DNN
picks the most discriminative wavelet sub-bands out of all
the sub-bands, with which we retrain our DNN, resulting
in a precise detection of morphed images when inference is
achieved on a probe image. The efﬁcacy of our deep morph
detector which is enhanced by structured group lasso is val-
idated through experiments on three facial morph image
databases, i.e., VISAPP17, LMA, and MorGAN.

Event-driven Re-Id: A New Benchmark and Method Towards
The large-scale use of surveillance cameras in public
spaces raised severe concerns about an individual privacy
breach. Introducing privacy and security in video surveil-
lance systems, primarily in person re-identification (re-id),
is quite challenging. Event cameras are novel sensors,
which only respond to brightness changes in the scene.
This characteristic makes event-based vision sensors vi-
able for privacy-preserving in video surveillance. Integrat-
ing privacy into the person re-id; this work investigates
the possibility of performing person re-id with the event-
camera network for the first time. We transform the asyn-
chronous events stream generated by an event camera into
synchronous image-like representations to leverage deep
learning models and then evaluate how complex the re-id
problem is with this new sensor modality. Interestingly, such
event-based representations contain meaningful spatial de-
tails which are very similar to standard edges and contours.
We use two different representations, image-like represen-
tation and their transformation to polar coordinates (which
carry more distinct edge patterns). Finally, we train a per-
son re-id model on such images to demonstrate the feasibil-
ity of performing event-driven re-id. We evaluate the per-
formance of our approach and produce baseline results on
two synthetic datasets (generated from publicly available
datasets, SAIVT and DukeMTMC-reid).

AirCamRTM: Enhancing Vehicle Detection for Efﬁcient Aerial Camera-based
Efﬁcient road trafﬁc monitoring is playing a fundamen-
tal role in successfully resolving trafﬁc congestion in cities.
Unmanned Aerial Vehicles (UAVs) or drones equipped with
cameras are an attractive proposition to provide ﬂexible
and infrastructure-free trafﬁc monitoring. However, real-
time trafﬁc monitoring from UAV imagery poses several
challenges, due to the large image sizes and presence of
non-relevant targets. In this paper, we propose the AirCam-
RTM framework that combines road segmentation and ve-
hicle detection to focus only on relevant vehicles, which as
a result, improves the monitoring performance by ∼ 2×
and provides ∼ 18% accuracy improvement. Furthermore,
through a real experimental setup we qualitatively evalu-
ate the performance of the proposed approach, and also
demonstrate how it can be used for real-time trafﬁc mon-
itoring using UAVs.

Fight Detection from Still Images in the Wild
Detecting ﬁghts from still images shared on social media
is an important task required to limit the distribution of vi-
olent scenes in order to prevent their negative effects. For
this reason, in this study, we address the problem of ﬁght
detection from still images collected from the web and so-
cial media. We explore how well one can detect ﬁghts from
just a single still image. We also propose a new dataset,
named Social Media Fight Images (SMFI), comprising real-
world images of ﬁght actions. Results of the extensive exper-
iments on the proposed dataset show that ﬁght actions can
be recognized successfully from still images. That is, even
without exploiting the temporal information, it is possible
to detect ﬁghts with high accuracy by utilizing appearance
only. We also perform cross-dataset experiments to eval-
uate the representation capacity of the collected dataset.
These experiments indicate that, as in the other computer
vision problems, there exists a dataset bias for the ﬁght
recognition problem. Although the methods achieve close
to 100% accuracy when trained and tested on the same
ﬁght dataset, the cross-dataset accuracies are signiﬁcantly
lower, i.e., around 70% when more representative datasets
are used for training. SMFI dataset is found to be one of
the two most representative datasets among the utilized ﬁve
ﬁght datasets.

An Investigation of Critical Issues in Bias Mitigation Techniques
A critical problem in deep learning is that systems learn
inappropriate biases, resulting in their inability to perform
well on minority groups. This has led to the creation of mul-
tiple algorithms that endeavor to mitigate bias. However, it
is not clear how effective these methods are. This is because
study protocols differ among papers, systems are tested on
datasets that fail to test many forms of bias, and systems
have access to hidden knowledge or are tuned speciﬁcally to
the test set. To address this, we introduce an improved evalu-
ation protocol, sensible metrics, and a new dataset, which
enables us to ask and answer critical questions about bias
mitigation algorithms. We evaluate seven state-of-the-art
algorithms using the same network architecture and hyper-
parameter selection policy across three benchmark datasets.
We introduce a new dataset called Biased MNIST that en-
ables assessment of robustness to multiple bias sources. We
use Biased MNIST and a visual question answering (VQA)
benchmark to assess robustness to hidden biases. Rather
than only tuning to the test set distribution, we study robust-
ness across different tuning distributions, which is critical
because for many applications the test distribution may not
be known during development. We ﬁnd that algorithms ex-
ploit hidden biases, are unable to scale to multiple forms
of bias, and are highly sensitive to the choice of tuning set.
Based on our ﬁndings, we implore the community to adopt
more rigorous assessment of future bias mitigation methods.
All data, code, and results are publicly available1.

Real-time Bangla License Plate Recognition System for Low Resource
Automatic License Plate Recognition systems aim to pro-
vide a solution for detecting, localizing, and recognizing
license plate characters from vehicles appearing in video
frames. However, deploying such systems in the real world
requires real-time performance in low-resource environ-
ments.
In our paper, we propose a two-stage detection
pipeline paired with Vision API that provides real-time in-
ference speed along with consistently accurate detection
and recognition performance. We used a haar-cascade
classifier as a filter on top of our backbone MobileNet
SSDv2 detection model. This reduces inference time by
only focusing on high confidence detections and using them
for recognition. We also impose a temporal frame separa-
tion strategy to distinguish between multiple vehicle license
plates in the same clip. Furthermore, there are no publicly
available Bangla license plate datasets, for which we cre-
ated an image dataset and a video dataset containing li-
cense plates in the wild. We trained our models on the image
dataset and achieved an AP0.5 score of 86% and tested our
pipeline on the video dataset and observed reasonable de-
tection and recognition performance (82.7% detection rate,
and 60.8% OCR F1 score) with real-time processing speed
(27.2 frames per second).

Attack Agnostic Detection of Adversarial Examples via
Whilst adversarial attack detection has received consid-
erable attention, it remains a fundamentally challenging
problem from two perspectives. First, while threat mod-
els can be well-defined, attacker strategies may still vary
widely within those constraints. Therefore, detection should
be considered as an open-set problem, standing in con-
trast to most current detection approaches. These methods
take a closed-set view and train binary detectors, thus bi-
asing detection toward attacks seen during detector train-
ing. Second, limited information is available at test time
and typically confounded by nuisance factors including the
label and underlying content of the image. We address
these challenges via a novel strategy based on random sub-
space analysis. We present a technique that utilizes prop-
erties of random projections to characterize the behavior
of clean and adversarial examples across a diverse set of
subspaces. The self-consistency (or inconsistency) of model
activations is leveraged to discern clean from adversar-
ial examples. Performance evaluations demonstrate that
our technique (AU C ∈ [0.92, 0.98]) outperforms compet-
ing detection strategies (AU C ∈ [0.30, 0.79]), while re-
maining truly agnostic to the attack strategy (for both tar-
geted/untargeted attacks). It also requires significantly less
calibration data (composed only of clean examples) than
competing approaches to achieve this performance.

A Personalized Benchmark for Face Anti-spoofing
Thanks to their ease-of-use and effectiveness, face au-
thentication systems are nowadays ubiquitous in electronic
devices to control access to protected data. However, the
widespread adoption of such systems comes with security
and reliability issues. This is because spoofs of face im-
ages can be easily fabricated to deceive the recognition
systems. Hence, there is a need to integrate the user iden-
tification system with a robust face anti-spoofing element,
which has the goal to detect whether a queried face image
is a spoof or live. Most contemporary face anti-spoofing
systems only rely on the query image to accept or reject
tentative access. In real-world scenarios, however, face au-
thentication systems often have an initial enrollment step
where a few live images of the user are recorded and stored
for identification purposes [23, 18, 33].
In this paper,
we present a complementary approach to augment exist-
ing face anti-spoofing benchmarks to account for enroll-
ment images associated with each query image. We apply
this strategy on two recently introduced datasets: CelebA-
Spoof [53] and SiW [29]. We showcase how existing anti-
spoofing models can be easily personalized using the sub-
ject’s enrollment data, and we evaluate the effectiveness of
the enhanced methods on the newly proposed datasets splits
CelebA-Spoof-Enroll and SiW-Enroll.

DIOR: DIstill Observations to Representations for
Multi-object tracking (MOT) has long been a crucial
topic in the field of autonomous driving and security moni-
toring. With the saturation of the bounding-box-based MOT
algorithms in recent years, a new task to track objects
with instance segmentation, called multi-object tracking
and segmentation (MOTS), provides a finer level of scene
understanding and introduces potential improvements in
tracking accuracy.
In this paper, we introduce a video-
based MOTS framework, named DIstill Observations to
Representations (DIOR). A feature distiller is designed to
extract and balance the comprehensive object representa-
tions: 1) the temporal distiller aggregates context informa-
tion for consistency of features and smoothness of predic-
tion longitudinally; 2) the spatial distiller on the target of
interest within each bounding box removes ambiguity and
irrelevance of background in the learned features. The sub-
sequent tracking steps start with Hungarian matching based
on feature similarity and masks continuity, which is effi-
cient and straightforward. In addition, we propose short-
term retrieval (STR) and long-term re-identification (re-ID)
modules to avoid missing associations due to failures in de-
tection or possible occlusion. Our method achieves state-
of-the-art performance in both MOTS20 and KITTI-MOTS
benchmarks.

CeyMo: See More on Roads - A Novel Benchmark Dataset for Road Marking
In this paper, we introduce a novel road marking bench-
mark dataset for road marking detection, addressing the
limitations in the existing publicly available datasets such
as lack of challenging scenarios, prominence given to lane
markings, unavailability of an evaluation script, lack of an-
notation formats and lower resolutions. Our dataset con-
sists of 2887 total images with 4706 road marking instances
belonging to 11 classes. The images have a high resolution
of 1920 × 1080 and capture a wide range of traffic, light-
ing and weather conditions. We provide road marking an-
notations in polygons, bounding boxes and pixel-level seg-
mentation masks to facilitate a diverse range of road mark-
ing detection algorithms. The evaluation metrics and the
evaluation script we provide, will further promote direct
comparison of novel approaches for road marking detec-
tion with existing methods. Furthermore, we evaluate the
effectiveness of using both instance segmentation and object
detection based approaches for the road marking detection
task. Speed and accuracy scores for two instance segmen-
tation models and two object detector models are provided
as a performance baseline for our benchmark dataset. The
dataset and the evaluation script is publicly available1.

CharacterGAN: Few-Shot Keypoint Character Animation and Reposing
We introduce CharacterGAN, a generative model that
can be trained on only a few samples (8 – 15) of a given
character. Our model generates novel poses based on key-
point locations, which can be modiﬁed in real time while
providing interactive feedback, allowing for intuitive repos-
ing and animation. Since we only have very limited training
samples, one of the key challenges lies in how to address
(dis)occlusions, e.g. when a hand moves behind or in front
of a body. To address this, we introduce a novel layering
approach which explicitly splits the input keypoints into dif-
ferent layers which are processed independently. These lay-
ers represent different parts of the character and provide
a strong implicit bias that helps to obtain realistic results
even with strong (dis)occlusions. To combine the features
of individual layers we use an adaptive scaling approach
conditioned on all keypoints. Finally, we introduce a mask
connectivity constraint to reduce distortion artifacts that oc-
cur with extreme out-of-distribution poses at test time. We
show that our approach outperforms recent baselines and
creates realistic animations for diverse characters. We also

Cleaning Noisy Labels by Negative Ensemble Learning for Source-Free
Conventional Unsupervised Domain Adaptation (UDA)
methods presume source and target domain data to be si-
multaneously available during training. Such an assump-
tion may not hold in practice, as source data is often in-
accessible (e.g., due to privacy reasons). On the contrary,
a pre-trained source model is usually available, which per-
forms poorly on target due to the well-known domain shift
problem. This translates into a significant amount of mis-
classifications, which can be interpreted as structured noise
affecting the inferred target pseudo-labels. In this work, we
cast UDA as a pseudo-label refinery problem in the chal-
lenging source-free scenario. We propose Negative Ensem-
ble Learning (NEL) technique, a unified method for adap-
tive noise filtering and progressive pseudo-label refinement.
NEL is devised to tackle noisy pseudo-labels by enhancing
diversity in ensemble members with different stochastic (i)
input augmentation and (ii) feedback. The latter is achieved
by leveraging the novel concept of Disjoint Residual Labels,
which allow propagating diverse information to the differ-
ent members. Eventually, a single model is trained with the
refined pseudo-labels, which leads to a robust performance
on the target domain. Extensive experiments show that the
proposed method achieves state-of-the-art performance on
major UDA benchmarks, such as Digit5, PACS, Visda-C,
and DomainNet, without using source data samples at all.

Contextual Proposal Network for Action Localization
This paper investigates the problem of Temporal Ac-
tion Proposal (TAP) generation, which aims to provide a
set of high-quality video segments that potentially contain
actions events locating in long untrimmed videos. Based
on the goal to distill available contextual information, we
introduce a Contextual Proposal Network (CPN) compos-
ing of two context-aware mechanisms. The ﬁrst mecha-
nism, i.e., feature enhancing, integrates the inception-like
module with long-range attention to capture the multi-scale
temporal contexts for yielding a robust video segment rep-
resentation. The second mechanism, i.e., boundary scor-
ing, employs the bi-directional recurrent neural networks
(RNN) to capture bi-directional temporal contexts that ex-
plicitly model actionness, background, and conﬁdence of
proposals. While generating and scoring proposals, such
bi-directional temporal contexts are helpful to retrieve high-
quality proposals of low false positives for covering the
video action instances. We conduct experiments on two
challenging datasets of ActivityNet-1.3 and THUMOS-14
to demonstrate the effectiveness of the proposed Contex-
tual Proposal Network (CPN). In particular, our method re-
spectively surpasses state-of-the-art TAP methods by 1.54%
AUC on ActivityNet-1.3 test split and by 0.61% AR@200 on
THUMOS-14 dataset.

CrossLocate: Cross-modal Large-scale Visual Geo-Localization
We propose a novel approach to visual geo-localization
in natural environments. This is a challenging problem due
to vast localization areas, the variable appearance of out-
door environments and the scarcity of available data.
In
order to make the research of new approaches possible,
we first create two databases containing “synthetic” im-
ages of various modalities. These image modalities are ren-
dered from a 3D terrain model and include semantic seg-
mentations, silhouette maps and depth maps. By combining
the rendered database views with existing datasets of pho-
tographs (used as “queries” to be localized), we create a
unique benchmark for visual geo-localization in natural en-
vironments, which contains correspondences between query
photographs and rendered database imagery. The dis-
tinct ability to match photographs to synthetically rendered
databases defines our task as “cross-modal”. On top of this
benchmark, we provide thorough ablation studies analysing
the localization potential of the database image modalities.
We reveal the depth information as the best choice for out-
door localization. Finally, based on our observations, we
carefully develop a fully-automatic method for large-scale
cross-modal localization using image retrieval. We demon-
strate its localization performance outdoors in the entire
state of Switzerland. Our method reveals a large gap be-
tween operating within a single image domain (e.g. pho-
tographs) and working across domains (e.g. photographs
matched to rendered images), as gained knowledge is not
transferable between the two. Moreover, we show that mod-
ern localization methods fail when applied to such a cross-
modal task and that our method achieves significantly better
results than state-of-the-art approaches. The datasets, code
and trained models are available on the project website:
http://cphoto.fit.vutbr.cz/crosslocate/.

Danish Fungi 2020 – Not Just Another Image Recognition Dataset
We introduce a novel fine-grained dataset and bench-
mark, the Danish Fungi 2020 (DF20). The dataset, con-
structed from observations submitted to the Atlas of Dan-
ish Fungi, is unique in its taxonomy-accurate class labels,
small number of errors, highly unbalanced long-tailed class
distribution, rich observation metadata, and well-defined
class hierarchy. DF20 has zero overlap with ImageNet, al-
lowing unbiased comparison of models fine-tuned from pub-
licly available ImageNet checkpoints. The proposed evalu-
ation protocol enables testing the ability to improve clas-
sification using metadata – e.g. precise geographic loca-
tion, habitat, and substrate, facilitates classifier calibra-
tion testing, and finally allows to study the impact of the
device settings on the classification performance. Experi-
ments using Convolutional Neural Networks (CNN) and the
recent Vision Transformers (ViT) show that DF20 presents
a challenging task. Interestingly, ViT achieves results su-
perior to CNN baselines with 80.45% accuracy and 0.743
macro F1 score, reducing the CNN error by 9% and 12% re-
spectively. A simple procedure for including metadata into
the decision process improves the classification accuracy
by more than 2.95 percentage points, reducing the error
rate by 15%. The source code for all methods and exper-
iments is available at https://sites.google.com/
view/danish-fungi-dataset.

DAQ: Channel-Wise Distribution-Aware Quantization for Deep Image
Since the resurgence of deep neural networks (DNNs),
image super-resolution (SR) has recently seen a huge
progress in improving the quality of low resolution images,
however at the great cost of computations and resources.
Recently, there has been several efforts to make DNNs more
efficient via quantization. However, SR demands pixel-level
accuracy in the system, it is more difficult to perform quan-
tization without significantly sacrificing SR performance.
To this end, we introduce a new ultra-low precision yet
effective quantization approach specifically designed for
SR. In particular, we observe that in recent SR networks,
each channel has different distribution characteristics. Thus
we propose a channel-wise distribution-aware quantiza-
tion scheme. Experimental results demonstrate that our pro-
posed quantization, dubbed Distribution-Aware Quantiza-
tion (DAQ), manages to greatly reduce the computational
and resource costs without the significant sacrifice in SR
performance, compared to other quantization methods.

Data InStance Prior (DISP) in Generative Adversarial Networks
Recent advances in generative adversarial networks
(GANs) have shown remarkable progress in generating
high-quality images. However, this gain in performance
depends on the availability of a large amount of train-
ing data.
In limited data regimes, training typically di-
verges, and therefore the generated samples are of low
quality and lack diversity. Previous works have addressed
training in low data setting by leveraging transfer learn-
ing and data augmentation techniques. We propose a novel
transfer learning method for GANs in the limited data do-
main by leveraging informative data prior derived from self-
supervised/supervised pre-trained networks trained on a di-
verse source domain. We perform experiments on several
standard vision datasets using various GAN architectures
(BigGAN, SNGAN, StyleGAN2) to demonstrate that the pro-
posed method effectively transfers knowledge to domains
with few target images, outperforming existing state-of-the-
art techniques in terms of image quality and diversity. We
also show the utility of data instance prior in large-scale
unconditional image generation.

Dataset Knowledge Transfer for Class-Incremental Learning without Memory
Incremental learning enables artificial agents to learn
from sequential data. While important progress was made
by exploiting deep neural networks, incremental learning
remains very challenging. This is particularly the case
when no memory of past data is allowed and catastrophic
forgetting has a strong negative effect. We tackle class-
incremental learning without memory by adapting predic-
tion bias correction, a method which makes predictions of
past and new classes more comparable.
It was proposed
when a memory is allowed and cannot be directly used
without memory, since samples of past classes are required.
We introduce a two-step learning process which allows the
transfer of bias correction parameters between reference
and target datasets. Bias correction is first optimized of-
fline on reference datasets which have an associated val-
idation memory. The obtained correction parameters are
then transferred to target datasets, for which no memory is
available. The second contribution is to introduce a finer
modeling of bias correction by learning its parameters per
incremental state instead of the usual past vs. new class
modeling. The proposed dataset knowledge transfer is ap-
plicable to any incremental method which works without
memory. We test its effectiveness by applying it to four ex-
isting methods. Evaluation with four target datasets and
different configurations shows consistent improvement, with
practically no computational and memory overhead.

Deep Online Fused Video Stabilization
We present a deep neural network (DNN) that uses both
sensor data (gyroscope) and image content (optical flow)
to stabilize videos through unsupervised learning. The net-
work fuses optical flow with real/virtual camera pose histo-
ries into a joint motion representation. Next, the LSTM cell
infers the new virtual camera pose, which is used to gen-
erate a warping grid that stabilizes the video frames. We
adopt a relative motion representation as well as a multi-
stage training strategy to optimize our model without any
supervision. To the best of our knowledge, this is the first
DNN solution that adopts both sensor data and image con-
tent for video stabilization. We validate the proposed frame-
work through ablation studies and demonstrate that the pro-
posed method outperforms the state-of-art alternative solu-
tions via quantitative evaluations and a user study. Check
out our video results, code and dataset at our website.

Detail Preserving Residual Feature Pyramid Modules for Optical Flow
Feature pyramids and iterative refinement have recently
led to great progress in optical flow estimation. However,
downsampling in feature pyramids can cause blending of
foreground objects with the background, which will mislead
subsequent decisions in the iterative processing. The results
are missing details especially in the flow of thin and of small
structures. We propose a novel Residual Feature Pyramid
Module (RFPM) which retains important details in the fea-
ture map without changing the overall iterative refinement
design of the optical flow estimation. RFPM incorporates
a residual structure between multiple feature pyramids into
a downsampling module that corrects the blending of ob-
jects across boundaries. We demonstrate how to integrate
our module with two state-of-the-art iterative refinement ar-
chitectures. Results show that our RFPM visibly reduces
flow errors and improves state-of-art performance in the
clean pass of Sintel, and is one of the top-performing meth-
ods in KITTI. According to the particular modular struc-
ture of RFPM, we introduce a special fine-tuning approach
that can dramatically decrease the training time compared
to a typical full optical flow training schedule on multiple
datasets.

DG-Labeler and DGL-MOTS Dataset:
Multi-object tracking and segmentation (MOTS) is a crit-
ical task for autonomous driving applications. The existing

Improving Person Re-Identiﬁcation with Temporal Constraints
In this paper we introduce an image-based person re-
identiﬁcation dataset collected across ﬁve non-overlapping
camera views in the large and busy airport in Dublin, Ire-
land. Unlike all publicly available image-based datasets,
our dataset contains timestamp information in addition to
frame number, and camera and person IDs. Also our dataset
has been fully anonymized to comply with modern data
privacy regulations. We apply state-of-the-art person re-
identiﬁcation models to our dataset and show that by lever-
aging the available timestamp information we are able to
achieve a signiﬁcant gain of 37.43% in mAP and a gain of
30.22% in Rank1 accuracy. We also propose a Bayesian tem-
poral re-ranking post-processing step, which further adds
a 10.03% gain in mAP and 9.95% gain in Rank1 accu-
racy metrics. This work on combining visual and tempo-
ral information is not possible on other image-based per-
son re-identiﬁcation datasets. We believe that the proposed
new dataset will enable further development of person re-
identiﬁcation research for challenging real-world applica-
tions.

Digital and Physical-World Attacks on Remote Pulse Detection
Remote photoplethysmography (rPPG) is a technique for
estimating blood volume changes from reflected light with-
out the need for a contact sensor. We present the first ex-
amples of presentation attacks in the digital and physical
domains on rPPG from face video. Digital attacks are eas-
ily performed by adding imperceptible periodic noise to the
input videos. Physical attacks are performed with illumi-
nation from visible spectrum LEDs placed in close prox-
imity to the face, while still being difficult to perceive with
the human eye. We also show that our attacks extend be-
yond medical applications, since the method can effectively
generate a strong periodic pulse on 3D-printed face masks,
which presents difficulties for pulse-based face presentation
attack detection (PAD). The paper concludes with ideas for
using this work to improve robustness of rPPG methods and
pulse-based face PAD.

Evaluating the Robustness of Semantic Segmentation for Autonomous Driving
Deep learning and convolutional neural networks allow
achieving impressive performance in computer vision tasks,
such as object detection and semantic segmentation (SS).
However, recent studies have shown evident weaknesses of
such models against adversarial perturbations. In a real-
world scenario instead, like autonomous driving, more at-
tention should be devoted to real-world adversarial exam-
ples (RWAEs), which are physical objects (e.g., billboards
and printable patches) optimized to be adversarial to the
entire perception pipeline. This paper presents an in-depth
evaluation of the robustness of popular SS models by test-
ing the effects of both digital and real-world adversarial
patches. These patches are crafted with powerful attacks
enriched with a novel loss function. Firstly, an investiga-
tion on the Cityscapes dataset is conducted by extending the
Expectation Over Transformation (EOT) paradigm to cope
with SS. Then, a novel attack optimization, called scene-
speciﬁc attack, is proposed. Such an attack leverages the
CARLA driving simulator to improve the transferability of
the proposed EOT-based attack to a real 3D environment.
Finally, a printed physical billboard containing an adver-
sarial patch was tested in an outdoor driving scenario to
assess the feasibility of the studied attacks in the real world.
Exhaustive experiments revealed that the proposed attack
formulations outperform previous work to craft both digi-
tal and real-world adversarial patches for SS. At the same
time, the experimental results showed how these attacks are
notably less effective in the real world, hence questioning
the practical relevance of adversarial attacks to SS models
for autonomous/assisted driving.
1. Introduction

FASSST: Fast Attention Based Single-Stage Segmentation Net for Real-Time
Real-time instance segmentation is crucial in various
AI applications. This work designs a network named Fast
Attention based Single-Stage Segmentation NeT (FASSST)
that performs instance segmentation with video-grade speed.
Using an instance attention module (IAM), FASSST quickly
locates target instances and segments with region of interest
(ROI) feature fusion (RFF) aggregating ROI features from
pyramid mask layers. The module employs an efﬁcient single-
stage feature regression, straight from features to instance
coordinates and class probabilities. Experiments on COCO
and CityScapes datasets show that FASSST achieves state-of-
the-art performance under competitive accuracy: real-time
inference of 47.5FPS on a GTX1080Ti GPU and 5.3FPS on
a Jetson Xavier NX board with only 71.6GFLOPs.

FastAno: Fast Anomaly Detection
Video anomaly detection has gained significant attention
due to the increasing requirements of automatic monitoring
for surveillance videos. Especially, the prediction based ap-
proach is one of the most studied methods to detect anoma-
lies by predicting frames that include abnormal events in the
test set after learning with the normal frames of the training
set. However, a lot of prediction networks are computation-
ally expensive owing to the use of pre-trained optical flow
networks, or fail to detect abnormal situations because of
their strong generative ability to predict even the anomalies.
To address these shortcomings, we propose spatial rotation
transformation (SRT) and temporal mixing transformation
(TMT) to generate irregular patch cuboids within normal
frame cuboids in order to enhance the learning of normal
features. Additionally, the proposed patch transformation
is used only during the training phase, allowing our model
to detect abnormal frames at fast speed during inference.
Our model is evaluated on three anomaly detection bench-
marks, achieving competitive accuracy and surpassing all
the previous works in terms of speed.
1. Introduction

Few-Shot Object Detection by Attending to Per-Sample-Prototype
Few-shot object detection aims to detect instances of spe-
cific categories in a query image with only a handful of
support samples. Although this takes less effort than ob-
taining enough annotated images for supervised object de-
tection, it results in a far inferior performance compared
to the conventional object detection methods. In this pa-
per, we propose a meta-learning-based approach that con-
siders the unique characteristics of each support sample.
Rather than simply averaging the information of the support
samples to generate a single prototype per category, our
method can better utilize the information of each support
sample by treating each support sample as an individual
prototype. Specifically, we introduce two types of attention
mechanisms for aggregating the query and support feature
maps. The first is to refine the information of few-shot sam-
ples by extracting shared information between the support
samples through attention. Second, each support sample is
used as a class code to leverage the information by com-
paring similarities between each support feature and query
features. Our proposed method is complementary to the pre-
vious methods, making it easy to plug and play for further
improvement. We have evaluated our method on PASCAL
VOC and COCO benchmarks, and the results verify the
effectiveness of our method. In particular, the advantages
of our method are maximized when there is more diversity
among support data.

Forgery Detection by Internal Positional Learning of Demosaicing Traces
We propose 4Point (Forensics with Positional Internal
Training), an unsupervised neural network trained to assess
the consistency of the image colour mosaic to ﬁnd forgeries.
Positional learning trains the model to learn the modulo-2
position of pixels, leveraging the translation-invariance of
CNN to replicate the underlying mosaic and its potential
inconsistencies.
Internal learning on a single potentially
forged image improves adaption and robustness to varied
post-processing and counter-forensics measures. This solu-
tion beats existing mosaic detection methods, is more robust
to various post-processing and counter-forensic artefacts
such as JPEG compression, and can exploit traces to which
state-of-the-art generic neural networks are blind. Check
qbammey.github.io/4point for the code.

Video representation learning through prediction for online object detection
We present a video representation learning framework
for real-time video object detection. Our approach is based
on the interesting observation that a powerful prior knowl-
edge of video context helps to improve object recognition,
and it can be acquired via learning video representations
through stochastic video prediction. Our proposed frame-
work utilizes the stochastic video prediction into object de-
tection so that we first acquire a prior knowledge of videos
to have video representations and then adjust them to ob-
ject detection to improve the accuracy. We validate our
proposed method on ImageNet VID and VisDrone-VID2019
datasets to demonstrate the effectiveness of video represen-
tation learning via future video prediction.
In particular,
our extensive experiments on ImageNet VID show that our
approach achieves 73.1% mAP at 54 fps with ResNet-50 on
commercial GPUs.

Video representation learning through prediction for online object detection
We present a video representation learning framework
for real-time video object detection. Our approach is based
on the interesting observation that a powerful prior knowl-
edge of video context helps to improve object recognition,
and it can be acquired via learning video representations
through stochastic video prediction. Our proposed frame-
work utilizes the stochastic video prediction into object de-
tection so that we first acquire a prior knowledge of videos
to have video representations and then adjust them to ob-
ject detection to improve the accuracy. We validate our
proposed method on ImageNet VID and VisDrone-VID2019
datasets to demonstrate the effectiveness of video represen-
tation learning via future video prediction.
In particular,
our extensive experiments on ImageNet VID show that our
approach achieves 73.1% mAP at 54 fps with ResNet-50 on
commercial GPUs.

GANs Spatial Control via Inference-Time Adaptive Normalization
ation effects across the image, while preserving the GAN’s
high visual quality.

Generating and Controlling Diversity in Image Search
In our society, generations of systemic biases have led
to some professions being more common among certain
genders and races. This bias is also reﬂected in image
search on stock image repositories and search engines,
e.g., a query like “male Asian administrative assistant” may
produce limited results. The pursuit of a utopian world
demands providing content users with an opportunity to
present any profession with diverse racial and gender char-
acteristics. The limited choice of existing content for cer-
tain combinations of profession, race, and gender presents
a challenge to content providers. Current research deal-
ing with bias in search mostly focuses on re-ranking algo-
rithms. However, these methods cannot create new content
or change the overall distribution of protected attributes in
photos. To remedy these problems, we propose a new task
of high-ﬁdelity image generation conditioning on multiple
attributes from imbalanced datasets. Our proposed task
poses new sets of challenges for the state-of-the-art Gen-
erative Adversarial Networks (GANs).
In this paper, we
also propose a new training framework to better address
the challenges. We evaluate our framework rigorously on a
real-world dataset and perform user studies that show our
model is preferable to the alternatives.

VQuAD: Video Question Answering Diagnostic Dataset
In this paper, we investigate the task of Video based
Question Answering. We provide a diagnostic dataset that
can be used to evaluate the extent of reasoning abilities of
various methods for solving this task. Previous datasets
proposed for this task do not have this ability. Our dataset
is large scale (around 1.3 million questions jointly for train
and test) and evaluates both the spatial and temporal prop-
erties and the relationship between various objects for these
properties. We evaluate state of the art language model
(BERT) as a baseline to understand the extent of correla-
tion based on language features alone. Other existing net-
works are then used to combine video features along with
language features for solving this task. Unfortunately, we
observe that the currently prevalent systems do not perform
signiﬁcantly better than the language baseline. We hypothe-
sise that this is due to our efforts in ensuring that no obvious
biases exist in this dataset and the dataset is balanced. To
make progress, the learning techniques needs to obtain an
ability to reason, going beyond basic correlation of biases.
This is an interesting and signiﬁcant challenge provided
through our work. We release our dataset and source code
for our baseline modules in the following webpage https:
//delta-lab-iitk.github.io/vquad/.

Improving Single-Image Defocus Deblurring: How Dual-Pixel Images Help
Many camera sensors use a dual-pixel (DP) design that
operates as a rudimentary light ﬁeld providing two sub-
aperture views of a scene in a single capture. The DP sen-
sor was developed to improve how cameras perform auto-
focus. Since the DP sensor’s introduction, researchers have
found additional uses for the DP data, such as depth es-
timation, reﬂection removal, and defocus deblurring. We
are interested in the latter task of defocus deblurring. In
particular, we propose a single-image deblurring network
that incorporates the two sub-aperture views into a multi-
task framework. Speciﬁcally, we show that jointly learning
to predict the two DP views from a single blurry input im-
age improves the network’s ability to learn to deblur the im-
age. Our experiments show this multi-task strategy achieves
+1dB PSNR improvement over state-of-the-art defocus de-
blurring methods. In addition, our multi-task framework al-
lows accurate DP-view synthesis (e.g., ∼ 39dB PSNR) from
the single input image. These high-quality DP views can be
used for other DP-based applications, such as reﬂection re-
moval. As part of this effort, we have captured a new dataset
of 7, 059 high-quality images to support our training for the
DP-view synthesis task.

In-Field Phenotyping Based on Crop Leaf and Plant Instance Segmentation
A detailed analysis of a plant’s phenotype in real field
conditions is critical for plant scientists and breeders to un-
derstand plant function. In contrast to traditional pheno-
typing performed manually, vision-based systems have the
potential for an objective and automated assessment with
high spatial and temporal resolution. One of such systems’
objectives is to detect and segment individual leaves of each
plant since this information correlates to the growth stage
and provides phenotypic traits, such as leaf count, cover-
age, and size.
In this paper, we propose a vision-based
approach that performs instance segmentation of individ-
ual crop leaves and associates each with its corresponding
crop plant in real fields. This enables us to compute relevant
basic phenotypic traits on a per-plant level. We employ a
convolutional neural network and operate directly on drone
imagery. The network generates two different representa-
tions of the input image that we utilize to cluster individual
crop leaf and plant instances. We propose a novel method to
compute clustering regions based on our network’s predic-
tions that achieves high accuracy. Furthermore, we com-
pare to other state-of-the-art approaches and show that our
system achieves superior performance. The source code of
our approach is available1.

Inferring the Class Conditional Response Map
for Weakly Supervised Semantic Segmentation
Image-level weakly supervised semantic segmentation
(WSSS) relies on class activation maps (CAMs) for pseudo
labels generation. As CAMs only highlight the most dis-
criminative regions of objects, the generated pseudo la-
bels are usually unsatisfactory to serve directly as supervi-
sion. To solve this, most existing approaches follow a multi-
training pipeline to reﬁne CAMs for better pseudo-labels,
which includes: 1) re-training the classiﬁcation model to
generate CAMs; 2) post-processing CAMs to obtain pseudo
labels; and 3) training a semantic segmentation model with
the obtained pseudo labels. However, this multi-training
pipeline requires complicated adjustment and additional
time. To address this, we propose a class-conditional in-
ference strategy and an activation aware mask reﬁnement
loss function to generate better pseudo labels without re-
training the classiﬁer. The class conditional inference-time
approach is presented to separately and iteratively reveal
the classiﬁcation network’s hidden object activation to gen-
erate more complete response maps. Further, our activa-
tion aware mask reﬁnement loss function introduces a novel
way to exploit saliency maps during segmentation training
and reﬁne the foreground object masks without suppress-
ing background objects. Our method achieves superior
WSSS results without requiring re-training of the classiﬁer.
https://github.com/weixuansun/InferCam

Intelligent Camera Selection Decisions for Target Tracking in a Camera Network
Camera Selection Decisions (CSD) are highly useful for
several applications in a multi-camera network. For exam-
ple, CSD beneﬁt multi-camera target tracking by reducing
the number of candidate cameras to look for the target’s
next location. The correct candidate cameras, decreases the
number of false Re-ID queries as well as the computation
time. Also, in multi-camera trajectory forecasting (MCTF)
to predict where a person will re-appear in the camera net-
work along with the transition time. These applications
require a large amount of annotated data for training. In
this paper, we use state-representation learning with a rein-
forcement learning based policy to effectively and efﬁciently
make camera selection decisions. We further demonstrate
that by using learned state representations, as opposed to
hand-crafted state variables, we are able to achieve state-
of-the-art results on camera selection, while reducing the
training time for the RL policy. Along with this, we use a
reward function that helps to reduce the amount of supervi-
sion in training the policy in a semi-supervised way. We re-
port our results on four datasets: NLPR MCT, DukeMTMC,
CityFlow, and WNMF dataset. We show that an RL pol-
icy reduces unnecessary Re-ID queries and therefore the
false alarms, scales well to larger camera networks, and
is target-agnostic.

Multiple Object Tracking and Forecasting: Jointly Predicting Current and
This paper introduces a joint learning architecture (JLA)
for multiple object tracking (MOT) and multiple object fore-
casting (MOF) in which the goal is to predict tracked ob-
jects’ current and future locations simultaneously. MOF is
a recent formulation of trajectory forecasting where the full
object bounding boxes are predicted rather than trajecto-
ries alone. Existing works separate multiple object track-
ing and multiple object forecasting. Such an approach can
propagate errors in tracking to forecasting. We propose a
joint learning architecture for multiple object tracking and
forecasting (MOTF). Our approach reduces the chances of
propagating tracking errors to the forecasting module. In
addition, we show, through a new data association step, that
forecasting predictions can be used for tracking objects dur-
ing occlusion. We adapt an existing MOT method to simul-
taneously predict current and future object locations and
confirm that JLA benefits both the MOT and MOF tasks.

Tae Soo Kim
In this paper, we release the Simulated Articulated VE-
hicles Dataset (SAVED) which contains images of synthetic
vehicles with moveable vehicle parts. SAVED consists of
images that are much more relevant for vehicle-related
pattern-recognition tasks than other popular pretraining
datasets such as ImageNet. Compared to a model initialized
with ImageNet weights, we show that a model pretrained us-
ing SAVED leads to much better performance when recog-
nizing vehicle parts and orientation directly from an image.
We also ﬁnd that a multi-task pretraining approach using
ﬁne-grained geometric signals available in SAVED leads to
signiﬁcant improvements in performance. By pretraining
on SAVED instead of ImageNet, we reduce the error rate of
one of the state of the art vehicle orientation estimators by
51.2% when tested on real images. We release SAVED and
instructions on its usage here1.

Unsupervised BatchNorm Adaptation (UBNA): A Domain Adaptation Method
In this paper we present a solution to the task of “unsu-
pervised domain adaptation (UDA) of a given pre-trained
semantic segmentation model without relying on any source
domain representations”. Previous UDA approaches for se-
mantic segmentation either employed simultaneous train-
ing of the model in the source and target domains, or they
relied on an additional network, replaying source domain
knowledge to the model during adaptation.
In contrast,
we present our novel Unsupervised BatchNorm Adaptation
(UBNA) method, which adapts a given pre-trained model to
an unseen target domain without using—beyond the existing
model parameters from pre-training—any source domain
representations (neither data, nor networks) and which can
also be applied in an online setting or using just a few un-
labeled images from the target domain in a few-shot man-
ner. Speciﬁcally, we partially adapt the normalization layer
statistics to the target domain using an exponentially de-
caying momentum factor, thereby mixing the statistics from
both domains. By evaluation on standard UDA benchmarks
for semantic segmentation we show that this is superior to a
model without adaptation and to baseline approaches using
statistics from the target domain only. Compared to stan-
dard UDA approaches we report a trade-off between per-
formance and usage of source domain representations.1

Interpretable Deep Learning-Based Forensic Iris Segmentation and Recognition
Iris recognition of living individuals is a mature biomet-
ric modality that has been adopted globally from govern-
mental ID programs, border crossing, voter registration and
de-duplication, to unlocking mobile phones. On the other
hand, the possibility of recognizing deceased subjects with
their iris patterns has emerged recently. In this paper, we
present an end-to-end deep learning-based method for post-
mortem iris segmentation and recognition with a special vi-
sualization technique intended to support forensic human
examiners in their efforts. The proposed postmortem iris
segmentation approach outperforms the state of the art and
– in addition to iris annulus, as in case of classical iris
segmentation methods – detects abnormal regions caused
by eye decomposition processes, such as furrows or irregu-
lar specular highlights present on the drying and wrinkling
cornea. The method was trained and validated with data ac-
quired from 171 cadavers, kept in mortuary conditions, and
tested on subject-disjoint data acquired from 259 deceased
subjects. To our knowledge, this is the largest corpus of
data used in postmortem iris recognition research to date.
The source code of the proposed method are offered with the
paper. The test data will be available through the National
Archive of Criminal Justice Data (NACJD) archives.

Late-resizing: A Simple but Effective Sketch Extraction Strategy
Automatic line-art colorization is a demanding research
field owing to its expensive and labor-intensive workload.
Learning-based approaches have lately emerged to improve
the quality of colorization. To handle the lack of paired
data in line art and color images, sketch extraction has
been widely adopted. This study primarily focuses on the
resizing process applied within the sketch extraction proce-
dure, which is essential for normalizing input sketches of
various sizes to the target size of the colorization model.
We first analyze the inherent risk in a conventional resiz-
ing strategy, i.e., early-resizing, which places the resizing
step before the line detection process to ensure the practi-
cality. Although the strategy is extensively used, it involves

Learnable Multi-level Frequency Decomposition and Hierarchical Attention
With the increased deployment of face recognition sys-
tems in our daily lives, face presentation attack detection
(PAD) is attracting much attention and playing a key role
in securing face recognition systems. Despite the great per-
formance achieved by the hand-crafted and deep-learning-
based methods in intra-dataset evaluations,
the perfor-
mance drops when dealing with unseen scenarios. In this
work, we propose a dual-stream convolution neural net-
works (CNNs) framework. One stream adapts four learn-
able frequency filters to learn features in the frequency
domain, which are less influenced by variations in sen-
sors/illuminations. The other stream leverages the RGB im-
ages to complement the features of the frequency domain.
Moreover, we propose a hierarchical attention module in-
tegration to join the information from the two streams at
different stages by considering the nature of deep features
in different layers of the CNN. The proposed method is eval-
uated in the intra-dataset and cross-dataset setups, and the
results demonstrate that our proposed approach enhances
the generalizability in most experimental setups in compar-
ison to state-of-the-art, including the methods designed ex-
plicitly for domain adaption/shift problems. We successfully
prove the design of our proposed PAD solution in a step-
wise ablation study that involves our proposed learnable
frequency decomposition, our hierarchical attention mod-
ule design, and the used loss function. Training codes and
pre-trained models are publicly released 1.

Learning Color Representations for Low-Light Image Enhancement
Color conveys important information about the visible
world. However, under low-light conditions, both pixel
intensity, as well as true color distribution, can be sig-
niﬁcantly shifted. Moreover, most of such distortions are
non-recoverable due to inverse problems.
In the present
study, we utilized recent advancements in learning-based
methods for low-light image enhancement. However, while
most “deep learning” methods aim to restore high-level
and object-oriented visual information, we hypothesized
that learning-based methods can also be used for restor-
ing color-based information. To address this question, we
propose a novel color representation learning method for
low-light image enhancement. More speciﬁcally, we used
a channel-aware residual network and a differentiable in-
tensity histogram to capture color features. Experimental
results using synthetic and natural datasets suggest that the
proposed learning scheme achieves state-of-the-art perfor-
mance. We conclude from our study that inter-channel de-
pendency and color distribution matching are crucial fac-
tors for learning color representations under low-light con-
ditions.

Learning Foreground-Background Segmentation from Improved Layered GANs
Deep learning approaches heavily rely on high-quality
human supervision which is nonetheless expensive, time-
consuming, and error-prone, especially for image segmen-
tation task.
In this paper, we propose a method to au-
tomatically synthesize paired photo-realistic images and
segmentation masks for the use of training a foreground-
background segmentation network. In particular, we learn
a generative adversarial network that decomposes an im-
age into foreground and background layers, and avoid triv-
ial decompositions by maximizing mutual information be-
tween generated images and latent variables. The improved
layered GANs can synthesize higher quality datasets from
which segmentation networks of higher performance can
be learned. Moreover, the segmentation networks are em-
ployed to stabilize the training of layered GANs in return,
which are further alternately trained with Layered GANs.
Experiments on a variety of single-object datasets show that
our method achieves competitive generation quality and
segmentation performance compared to related methods.

Detecting Arbitrary Intermediate Keypoints for Human Pose Estimation with
Most human pose estimation datasets have a ﬁxed set
of keypoints. Hence, trained models are only capable of
detecting these deﬁned points. Adding new points to the
dataset requires a full retraining of the model. We present
a model based on the Vision Transformer architecture that
can detect these ﬁxed points and arbitrary intermediate
points without any computational overhead during infer-
ence time. Furthermore, independently detected interme-
diate keypoints can improve analyses derived from the key-
points such as the calculation of body angles. Our approach
is based on TokenPose [9] and replaces the ﬁxed keypoint
tokens with an embedding of human readable keypoint vec-
tors to keypoint tokens. For ski jumpers, who beneﬁt from
intermediate detections especially of their skis, this model
achieves the same performance as TokenPose on the ﬁxed
keypoints and can detect any intermediate keypoint directly.

Class-aware Object Counting
Estimating the correct number of objects in a given nat-
ural scene is a common challenge in computer vision. Nat-
ural scenes usually contain multiple object categories and
varying object densities. Detection-based algorithms are
well suited for class-aware object counting and low object
counts. However, they underperform with high or varying
numbers of objects. To address this challenge, we propose
an end-to-end approach to enhance an existing detection-
based method with a multi-class density estimation branch.
The results of both branches are fed into a successive count-
estimation network, which estimates object counts for each
category. Although these numbers do not contain any lo-
calization information, they can be used as a valuable in-
dicator for verifying the exactness of the object detector re-
sults and improving its counting performance. In order to
demonstrate the effectiveness, we evaluate our method on
common object detection datasets.

Modeling dynamic target deformation in camera calibration
Most approaches to camera calibration rely on calibra-
tion targets of well-known geometry. During data acqui-
sition, calibration target and camera system are typically
moved w.r.t. each other, to allow image coverage and per-
spective versatility. We show that moving the target can
lead to small temporary deformations of the target, which
can introduce signiﬁcant errors into the calibration result.
While static inaccuracies of calibration targets have been
addressed in previous works, to our knowledge, none of the
existing approaches can capture time-varying, dynamic de-
formations. To achieve high-accuracy calibrations despite
moving the target, we propose a way to explicitly model dy-
namic target deformations in camera calibration. This is
achieved by using a low-dimensional deformation model
with only few parameters per image, which can be opti-
mized jointly with target poses and intrinsics. We demon-
strate the effectiveness of modeling dynamic deformations
using different calibration targets and show its signiﬁcance
in a structure-from-motion application.

MoESR: Blind Super-Resolution using Kernel-Aware Mixture of Experts
Modern deep learning super-resolution approaches have
achieved remarkable performance where the low-resolution
(LR) input is a degraded high-resolution (HR) image by
a fixed known kernel i.e. kernel-specific super-resolution
(SR). However, real images often vary in their degradation
kernels, thus a single kernel-specific SR approach does not
often produce accurate HR results. Recently, degradation-
aware networks are introduced to generate blind SR results
for unknown kernel conditions. They can restore images
for multiple blur kernels. However, they have to compro-
mise in quality compared to their kernel-specific counter-
parts. To address this issue, we propose a novel blind
SR method called Mixture of Experts Super-Resolution
(MoESR), which uses different experts for different degrada-
tion kernels. A broad space of degradation kernels is cov-
ered by kernel-specific SR networks (experts). We present
an accurate kernel prediction method (gating mechanism)
by evaluating the sharpness of images generated by ex-
perts. Based on the predicted kernel, our most suited ex-
pert network is selected for the input image. Finally, we
fine-tune the selected network on the test image itself to
leverage the advantage of internal learning. Our experi-
mental results on standard synthetic datasets and real im-
ages demonstrate that MoESR outperforms state-of-the-art
methods both quantitatively and qualitatively. Especially
for the challenging ×4 SR task, our PSNR improvement of
0.93 dB on the DIV2KRK dataset is substantial 1.

Multi-Domain Incremental Learning for Semantic Segmentation
Recent efforts in multi-domain learning for semantic seg-
mentation attempt to learn multiple geographical datasets
in a universal, joint model. A simple fine-tuning experi-
ment performed sequentially on three popular road scene
segmentation datasets demonstrates that existing segmenta-
tion frameworks fail at incrementally learning on a series of
visually disparate geographical domains. When learning a
new domain, the model catastrophically forgets previously
learned knowledge. In this work, we pose the problem of
multi-domain incremental learning for semantic segmenta-
tion. Given a model trained on a particular geographical
domain, the goal is to (i) incrementally learn a new geo-
graphical domain, (ii) while retaining performance on the
old domain, (iii) given that the previous domain’s dataset
is not accessible. We propose a dynamic architecture that
assigns universally shared, domain-invariant parameters to
capture homogeneous semantic features present in all do-
mains, while dedicated domain-specific parameters learn
the statistics of each domain. Our novel optimization strat-
egy helps achieve a good balance between retention of old
knowledge (stability) and acquiring new knowledge (plas-
ticity). We demonstrate the effectiveness of our proposed
solution on domain incremental settings pertaining to real-
world driving scenes from roads of Germany (Cityscapes),
the United States (BDD100k), and India (IDD). 1

Multi-level Attentive Adversarial Learning with Temporal Dilation for
Most existing works on unsupervised video domain
adaptation attempt to mitigate the distribution gap across
domains in frame and video levels. Such two-level distri-
bution alignment approach may suffer from the problems
of insufficient alignment for complex video data and mis-
alignment along the temporal dimension. To address these
issues, we develop a novel framework of Multi-level Atten-
tive Adversarial Learning with Temporal Dilation (MA2L-
TD). Given frame-level features as input, multi-level tem-
poral features are generated and multiple domain discrim-
inators are individually trained by adversarial learning for
them. For better distribution alignment, level-wise attention
weights are calculated by the degree of domain confusion in
each level. To mitigate the negative effect of misalignment,
features are aggregated with the attention mechanism deter-
mined by individual domain discriminators. Moreover, tem-
poral dilation is designed for sequential non-repeatability
to balance the computational efficiency and the possible
number of levels. Extensive experimental results show that
our proposed method outperforms the state of the art on
four benchmark datasets.1

Multi-Task Classification of Sewer Pipe Defects and Properties using a
The sewerage infrastructure is one of the most important
and expensive infrastructures in modern society. In order to
efficiently manage the sewerage infrastructure, automated
sewer inspection has to be utilized. However, while sewer
defect classification has been investigated for decades, little
attention has been given to classifying sewer pipe properties
such as water level, pipe material, and pipe shape, which
are needed to evaluate the level of sewer pipe deterioration.
In this work we classify sewer pipe defects and prop-
erties concurrently and present a novel decoder-focused
multi-task classification architecture Cross-Task Graph Neu-
ral Network (CT-GNN), which refines the disjointed per-task
predictions using cross-task information. The CT-GNN archi-
tecture extends the traditional disjointed task-heads decoder,
by utilizing a cross-task graph and unique class node em-
beddings. The cross-task graph can either be determined
a priori based on the conditional probability between the
task classes or determined dynamically using self-attention.
CT-GNN can be added to any backbone and trained end-to-
end at a small increase in the parameter count. We achieve
state-of-the-art performance on all four classification tasks
in the Sewer-ML dataset, improving defect classification and
water level classification by 5.3 and 8.0 percentage points,
respectively. We also outperform the single task methods
as well as other multi-task classification approaches while
introducing 50 times fewer parameters than previous model-
focused approaches. The code and models are available at
the project page http://vap.aau.dk/ctgnn.

Mutual Learning of Joint and Separate Domain Alignments for Multi-Source
Multi-Source Domain Adaptation (MSDA) aims at trans-
ferring knowledge from multiple labeled source domains to
beneﬁt the task in an unlabeled target domain. The chal-
lenges of MSDA lie in mitigating domain gaps and combin-
ing information from diverse source domains. In most ex-
isting methods, the multiple source domains can be jointly
or separately aligned to the target domain. In this work, we
consider that these two types of methods, i.e. joint and sepa-
rate domain alignments, are complementary and propose a
mutual learning based alignment network (MLAN) to com-
bine their advantages. Speciﬁcally, our proposed method
is composed of three components, i.e. a joint alignment
branch, a separate alignment branch, and a mutual learn-
ing objective between them. In the joint alignment branch,
the samples from all source domains and the target domain
are aligned together, with a single domain alignment goal,
while in the separate alignment branch, each source do-
main is individually aligned to the target domain. Finally,
by taking advantage of the complementarity of joint and
separate domain alignment mechanisms, mutual learning is
used to make the two branches learn collaboratively. Com-
pared with other existing methods, our proposed MLAN in-
tegrates information of different domain alignment mecha-
nisms and thus can mine rich knowledge from multiple do-
mains for better performance. The experiments on Domain-
Net, Ofﬁce-31, and Digits-ﬁve datasets demonstrate the ef-
fectiveness of our method.

Myope Models - Are face presentation attack detection models short-sighted?
Presentation attacks are recurrent threats to biometric
systems, where impostors attempt to bypass these systems.
Humans often use background information as contextual
cues for their visual system. Yet, regarding face-based sys-
tems, the background is often discarded, since face pre-
sentation attack detection (PAD) models are mostly trained
with face crops. This work presents a comparative study of
face PAD models (including multi-task learning, adversar-
ial training and dynamic frame selection) in two settings:
with and without crops. The results show that the perfor-
mance is consistently better when the background is present
in the images. The proposed multi-task methodology beats
the state-of-the-art results on the ROSE-Youtu dataset by
a large margin with an equal error rate of 0.2%. Fur-
thermore, we analyze the models’ predictions with Grad-
CAM++ with the aim to investigate to what extent the mod-
els focus on background elements that are known to be use-
ful for human inspection. From this analysis we can con-
clude that the background cues are not relevant across all
the attacks. Thus, showing the capability of the model to
leverage the background information only when necessary.

Neural Radiance Fields Approach to Deep Multi-View Photometric Stereo
We present a modern solution to the multi-view photo-
metric stereo problem (MVPS). Our work suitably exploits
the image formation model in a MVPS experimental setup
to recover the dense 3D reconstruction of an object from
images. We procure the surface orientation using a photo-
metric stereo (PS) image formation model and blend it with
a multi-view neural radiance field representation to recover
the object’s surface geometry. Contrary to the previous
multi-staged framework to MVPS, where the position, iso-
depth contours, or orientation measurements are estimated
independently and then fused later, our method is simple to
implement and realize. Our method performs neural ren-
dering of multi-view images while utilizing surface normals
estimated by a deep photometric stereo network. We ren-
der the MVPS images by considering the object’s surface
normals for each 3D sample point along the viewing di-
rection rather than explicitly using the density gradient in
the volume space via 3D occupancy information. We opti-
mize the proposed neural radiance field representation for
the MVPS setup efficiently using a fully connected deep net-
work to recover the 3D geometry of an object. Extensive
evaluation on the DiLiGenT-MV benchmark dataset shows
that our method performs better than the approaches that
perform only PS or only multi-view stereo (MVS) and pro-
vides comparable results against the state-of-the-art multi-
stage fusion methods.

No-Reference Image Quality Assessment via Transformers, Relative Ranking,
The goal of No-Reference Image Quality Assessment
(NR-IQA) is to estimate the perceptual image quality in ac-
cordance with subjective evaluations, it is a complex and
unsolved problem due to the absence of the pristine refer-
ence image. In this paper, we propose a novel model to ad-
dress the NR-IQA task by leveraging a hybrid approach that
benefits from Convolutional Neural Networks (CNNs) and
self-attention mechanism in Transformers to extract both lo-
cal and non-local features from the input image. We capture
local structure information of the image via CNNs, then to
circumvent the locality bias among the extracted CNNs fea-
tures and obtain a non-local representation of the image,
we utilize Transformers on the extracted features where we
model them as a sequential input to the Transformer model.
Furthermore, to improve the monotonicity correlation be-
tween the subjective and objective scores, we utilize the rel-
ative distance information among the images within each
batch and enforce the relative ranking among them. Last
but not least, we observe that the performance of NR-IQA
models degrades when we apply equivariant transforma-
tions (e.g. horizontal flipping) to the inputs. Therefore,
we propose a method that leverages self-consistency as a
source of self-supervision to improve the robustness of NR-
IQA models. Specifically, we enforce self-consistency be-
tween the outputs of our quality assessment model for each
image and its transformation (horizontally flipped) to utilize
the rich self-supervisory information and reduce the uncer-
tainty of the model. To demonstrate the effectiveness of our
work, we evaluate it on seven standard IQA datasets (both
synthetic and authentic) and show that our model achieves
state-of-the-art results on various datasets. 1

Normalizing Flow as a Flexible Fidelity Objective for Photo-Realistic
Super-resolution is an ill-posed problem, where a
ground-truth high-resolution image represents only one
possibility in the space of plausible solutions. Yet, the
dominant paradigm is to employ pixel-wise losses, such
as L1, which drive the prediction towards a blurry av-
erage. This leads to fundamentally conflicting objectives
when combined with adversarial losses, which degrades the
final quality. We address this issue by revisiting the L1
loss and show that it corresponds to a one-layer conditional
flow. Inspired by this relation, we explore general flows as
a fidelity-based alternative to the L1 objective. We demon-
strate that the flexibility of deeper flows leads to better vi-
sual quality and consistency when combined with adver-
sarial losses. We conduct extensive user studies for three
datasets and scale factors, where our approach is shown
to outperform state-of-the-art methods for photo-realistic
super-resolution. Code and trained models: git.io/AdFlow

Novel Ensemble Diversification Methods for Open-Set Scenarios
We revisit existing ensemble diversification approaches
and present two novel diversification methods tailored for
open-set scenarios. The first method uses a new loss, de-
signed to encourage models disagreement on outliers only,
thus alleviating the intrinsic accuracy-diversity trade-off.
The second method achieves diversity via automated fea-
ture engineering, by training each model to disregard in-
put features learned by previously trained ensemble models.
We conduct an extensive evaluation and analysis of the pro-
posed techniques on seven datasets that cover image clas-
sification, re-identification and recognition domains. We
compare to and demonstrate accuracy improvements over
the existing state-of-the-art ensemble diversification meth-
ods.

Novel-View Synthesis of Human Tourist Photos
We present a novel framework for performing novel-view
synthesis on human tourist photos. Given a tourist photo
from a known scene, we reconstruct the photo in 3D space
through modeling the human and the background indepen-
dently. We generate a deep buffer from a novel viewpoint
of the reconstruction and utilize a deep network to translate
the buffer into a photo-realistic rendering of the novel view.
We additionally present a method to relight the renderings,
allowing for relighting of both human and background to
match either the provided input image or any other. The
key contributions of our paper are: 1) a framework for per-
forming novel view synthesis on human tourist photos, 2)
an appearance transfer method for relighting of humans to
match synthesized backgrounds, and 3) a method for esti-
mating lighting properties from a single human photo. We
demonstrate the proposed framework on photos from two
different scenes of various tourists.

Occlusion Resistant Network for 3D Face Reconstruction
3D face reconstruction from a monocular face image is a
mathematically ill-posed problem. Recently, we observed a
surge of interest in deep learning-based approaches to ad-
dress the issue. These methods possess extreme sensitivity
towards occlusions. Thus, in this paper, we present a novel
context-learning-based distillation approach to tackle the
occlusions in the face images. Our training pipeline focuses
on distilling the knowledge from a pre-trained occlusion-
sensitive deep network. The proposed model learns the
context of the target occluded face image. Hence our ap-
proach uses a weak model (unsuitable for occluded face im-
ages) to train a highly robust network towards partially and
fully-occluded face images. We obtain a landmark accu-
racy of 0.77 against 5.84 of recent state-of-the-art-method
for real-life challenging facial occlusions. Also, we pro-
pose a novel end-to-end training pipeline to reconstruct 3D
faces from multiple variations of the target image per iden-
tity to emphasize the signiﬁcance of visible facial features
during learning. For this purpose, we leverage a novel com-
posite multi-occlusion loss function. Our multi-occlusion
per identity model shows a dip in the landmark error by
a large margin of 6.67 in comparison to a recent state-of-
the-art method. We deploy the occluded variations of the
CelebA validation dataset and AFLW2000-3D face dataset:
naturally-occluded and artiﬁcially occluded, for the com-
parisons. We comprehensively compare our results with
the other approaches concerning the accuracy of the recon-
structed 3D face mesh for occluded face images.

One-Class Learned Encoder-Decoder Network with Adversarial Context
Novelty detection is the task of recognizing samples that
do not belong to the distribution of the target class. During
training, the novelty class is absent, preventing the use of
traditional classification approaches. Deep autoencoders
have been widely used as a base of many novelty detec-
tion methods. In particular, context autoencoders have been
successful in the novelty detection task because of the more
effective representations they learn by reconstructing orig-
inal images from randomly masked images. However, a
significant drawback of context autoencoders is that ran-
dom masking fails to consistently cover important struc-
tures of the input image, leading to suboptimal representa-
tions - especially for the novelty detection task. In this pa-
per, to optimize input masking, we introduce a Mask Module
that learns to generate optimal masks and a Reconstruc-
tor that aims to reconstruct masked images. The networks
are trained in an adversarial setting in which the Mask
Module seeks to maximize the reconstruction error that the
Reconstructor is minimizing. When applied to novelty de-
tection, the proposed approach learns semantically richer
representations compared to context autoencoders and en-
hances novelty detection at test time through more optimal
masking. Novelty detection experiments on the MNIST and
CIFAR-10 image datasets demonstrate the proposed ap-
proach’s superiority over cutting-edge methods. In a fur-
ther experiment on the UCSD video dataset for novelty de-
tection, the proposed approach achieves a frame-level Area
Under the Curve (AUC) of 99.02% and an Equal Error Rate
(EER) of 5.4%, exceeding recent state-of-the-art models.
Code available at https://github.com/jewelltaylor/OLED.

Ortho-Shot: Low Displacement Rank Regularization with Data Augmentation
In few-shot classiﬁcation, the primary goal is to learn
representations from a few samples that generalize well for
novel classes.
In this paper, we propose an efﬁcient low
displacement rank (LDR) regularization strategy termed
Ortho-Shot; a technique that imposes orthogonal regular-
ization on the convolutional layers of a few-shot classi-
ﬁer, which is based on the doubly-block toeplitz (DBT)
matrix structure. The regularized convolutional layers of
the few-shot classiﬁer enhances model generalization and
intra-class feature embeddings that are crucial for few-shot
learning. Overﬁtting is a typical issue for few-shot mod-
els, the lack of data diversity inhibits proper model infer-
ence which weakens the classiﬁcation accuracy of few-shot
learners to novel classes.
In this regard, we broke down
the pipeline of the few-shot classiﬁer and established that
the support, query and task data augmentation collectively
alleviates overﬁtting in networks. With compelling results,
we demonstrated that combining a DBT-based low-rank or-
thogonal regularizer with data augmentation strategies, sig-
niﬁcantly boosts the performance of a few-shot classiﬁer.
We perform our experiments on the miniImagenet, CIFAR-
FS and Stanford datasets with performance values of about
5% when compared to state-of-the-art.
1. Introduction

Pixel-by-Pixel Cross-Domain Alignment for Few-Shot Semantic Segmentation
In this paper we consider the task of semantic segmen-
tation in autonomous driving applications. Specifically, we
consider the cross-domain few-shot setting where training
can use only few real-world annotated images and many
annotated synthetic images. In this context, aligning the do-
mains is made more challenging by the pixel-wise class im-
balance that is intrinsic in the segmentation and that leads
to ignoring the underrepresented classes and overfitting the
well represented ones. We address this problem with a novel
framework called Pixel-By-Pixel Cross-Domain Alignment
(PixDA). We propose a novel pixel-by-pixel domain adver-
sarial loss following three criteria: (i) align the source and
the target domain for each pixel, (ii) avoid negative trans-
fer on the correctly represented pixels, and (iii) regularize
the training of infrequent classes to avoid overfitting. The
pixel-wise adversarial training is assisted by a novel sample
selection procedure, that handles the imbalance between
source and target data, and a knowledge distillation strat-
egy, that avoids overfitting towards the few target images.
We demonstrate on standard synthetic-to-real benchmarks
that PixDA outperforms previous state-of-the-art methods
in (1-5)-shot settings.1

PoP-Net: Pose over Parts Network for Multi-Person 3D Pose Estimation from a
In this paper, a real-time method called PoP-Net is pro-
posed to predict multi-person 3D poses from a depth image.
PoP-Net learns to predict bottom-up part representations
and top-down global poses in a single shot. Speciﬁcally,
a new part-level representation, called Truncated Part Dis-
placement Field (TPDF), is introduced which enables an
explicit fusion process to unify the advantages of bottom-
up part detection and global pose detection. Meanwhile,
an effective mode selection scheme is introduced to auto-
matically resolve the conﬂicting cases between global pose
and part detections. Finally, due to the lack of high-quality
depth datasets for developing multi-person 3D pose estima-
tion, we introduce Multi-Person 3D Human Pose Dataset
(MP-3DHP) as a new benchmark. MP-3DHP is designed to
enable effective multi-person and background data augmen-
tation in model training, and to evaluate 3D human pose
estimators under uncontrolled multi-person scenarios. We
show that PoP-Net achieves the state-of-the-art results both
on MP-3DHP and on the widely used ITOP dataset, and
has signiﬁcant advantages in efﬁciency for multi-person
processing. MP-3DHP Dataset and the evaluation code
have been made available at: https://github.com/
oppo-us-research/PoP-Net.

Pose and Joint-Aware Action Recognition
Recent progress on action recognition has mainly fo-
cused on RGB and optical flow features.
In this paper,
we approach the problem of joint-based action recognition.
Unlike other modalities, constellation of joints and their
motion generate models with succinct human motion infor-
mation for activity recognition. We present a new model for
joint-based action recognition, which first extracts motion
features from each joint separately through a shared motion
encoder before performing collective reasoning. Our joint
selector module re-weights the joint information to select
the most discriminative joints for the task. We also propose
a novel joint-contrastive loss that pulls together groups of
joint features which convey the same action. We strengthen
the joint-based representations by using a geometry-aware
data augmentation technique which jitters pose heatmaps
while retaining the dynamics of the action. We show large
improvements over the current state-of-the-art joint-based
approaches on JHMDB, HMDB, Charades, AVA action
recognition datasets. A late fusion with RGB and Flow-
based approaches yields additional improvements. Our
model also outperforms the existing baseline on Mimetics,
a dataset with out-of-context actions.

Pose-guided Generative Adversarial Net for Novel View Action Synthesis
We focus on the problem of novel-view human action syn-
thesis. Given an action video, the goal is to generate the
same action from an unseen viewpoint. Naturally, novel
view video synthesis is more challenging than image syn-
thesis. It requires the synthesis of a sequence of realistic
frames with temporal coherency. Besides, transferring dif-
ferent actions to a novel target view requires awareness of
action category and viewpoint change simultaneously. To
address these challenges we propose a novel framework
named Pose-guided Action Separable Generative Adversar-
ial Net (PAS-GAN), which utilizes pose to alleviate the dif-
ﬁculty of this task. First, we propose a recurrent pose-
transformation module which transforms actions from the
source view to the target view and generates novel view
pose sequence in 2D coordinate space. Second, a well-
transformed pose sequence enables us to separate the ac-
tion and background in the target view. We employ a novel
local-global spatial transformation module to effectively
generate sequential video features in the target view using
these action and background features. Finally, the gener-
ated video features are used to synthesize human action with
the help of a 3D decoder. Moreover, to focus on dynamic
action in the video, we propose a novel multi-scale action-
separable loss which further improves the video quality.
We conduct extensive experiments on two large-scale multi-
view human action datasets, NTU-RGBD and PKU-MMD,
demonstrating the effectiveness of PAS-GAN which outper-
forms existing approaches. The codes and models will be
available on https://github.com/xhl-video/PAS-GAN.

Post-OCR Paragraph Recognition by Graph Convolutional Networks
We propose a new approach for paragraph recognition in
document images by spatial graph convolutional networks
(GCN) applied on OCR text boxes. Two steps, namely line
splitting and line clustering, are performed to extract para-
graphs from the lines in OCR results. Each step uses a β-
skeleton graph constructed from bounding boxes, where the
graph edges provide efficient support for graph convolution
operations. With pure layout input features, the GCN model
size is 3∼4 orders of magnitude smaller compared to R-
CNN based models, while achieving comparable or better
accuracies on PubLayNet and other datasets. Furthermore,
the GCN models show good generalization from synthetic
training data to real-world images, and good adaptivity for
variable document styles.

Quantified Facial Expressiveness for Affective Behavior Analytics
The quantified measurement of facial expressiveness is
crucial to analyze human affective behavior at scale. Un-
fortunately, methods for expressiveness quantification at the
video frame-level are largely unexplored, unlike the study of
discrete expression. In this work, we propose an algorithm
that quantifies facial expressiveness using a bounded, con-
tinuous expressiveness score using multimodal facial fea-
tures, such as action units (AUs), landmarks, head pose, and
gaze. The proposed algorithm more heavily weights AUs
with high intensities and large temporal changes. The pro-
posed algorithm can compute the expressiveness in terms
of discrete expression, and can be used to perform tasks
including facial behavior tracking and subjectivity quantifi-
cation in context. Our results on benchmark datasets show
the proposed algorithm is effective in terms of capturing
temporal changes and expressiveness, measuring subjective
differences in context, and extracting useful insight.

Reconstructing Training Data from Diverse ML Models by Ensemble Inversion
Model Inversion (MI), in which an adversary abuses ac-
cess to a trained Machine Learning (ML) model attempt-
ing to infer sensitive information about its original train-
ing data, has attracted increasing research attention. Dur-
ing MI, the trained model under attack (MUA) is usually
frozen and used to guide the training of a generator, such
as a Generative Adversarial Network (GAN), to reconstruct
the distribution of the original training data of that model.
This might cause leakage of original training samples, and
if successful, the privacy of dataset subjects will be at risk if
the training data contains Personally Identiﬁable Informa-
tion (PII). Therefore, an in-depth investigation of the poten-
tials of MI techniques is crucial for the development of cor-
responding defense techniques. High-quality reconstruc-
tion of training data based on a single model is challenging.
However, existing MI literature does not explore targeting
multiple models jointly, which may provide additional in-
formation and diverse perspectives to the adversary.

RGL-NET: A Recurrent Graph Learning framework for Progressive Part
Autonomous assembly of objects is an essential task in
robotics and 3D computer vision. It has been studied exten-
sively in robotics as a problem of motion planning, actuator
control and obstacle avoidance. However, the task of de-
veloping a generalized framework for assembly robust to
structural variants remains relatively unexplored. In this
work, we tackle this problem using a recurrent graph learn-
ing framework considering inter-part relations and the pro-
gressive update of the part pose. Our network can learn
more plausible predictions of shape structure by account-
ing for priorly assembled parts. Compared to the current
state-of-the-art, our network yields up to 10% improvement
in part accuracy and up to 15% improvement in connectiv-
ity accuracy on the PartNet [23] dataset. Moreover, our
resulting latent space facilitates exciting applications such
as shape recovery from the point-cloud components. We con-
duct extensive experiments to justify our design choices and
demonstrate the effectiveness of the proposed framework.

Monocular Depth Estimation Using Multi Scale Neural Network And Feature
Depth estimation from monocular images is a challenging
problem in computer vision. In this paper, we tackle this
problem using a novel network architecture using multi scale
feature fusion. Our network uses two different blocks, first
which uses different filter sizes for convolution and merges
all the individual feature maps. The second block uses di-
lated convolutions in place of fully connected layers thus
reducing computations and increasing the receptive field. We
present a new loss function for training the network which
uses a depth regression term, SSIM loss term and a multi-
nomial logistic loss term combined. We train and test our
network on Make 3D dataset, NYU Depth V2 dataset and
Kitti dataset using standard evaluation metrics for depth
estimation comprised of RMSE loss and SILog loss. Our
network outperforms previous state of the art methods with
lesser parameters.

Sandwich Batch Normalization:
We present Sandwich Batch Normalization (SaBN), a
frustratingly easy improvement of Batch Normalization
(BN) with only a few lines of code changes.
SaBN is
motivated by addressing the inherent feature distribution
heterogeneity that one can be identified in many tasks,
which can arise from data heterogeneity (multiple input
domains) or model heterogeneity (dynamic architectures,
model conditioning, etc.). Our SaBN factorizes the BN
affine layer into one shared sandwich affine layer, cas-
caded by several parallel independent affine layers. Con-
crete analysis reveals that, during optimization, SaBN pro-
motes balanced gradient norms while still preserving di-
verse gradient directions – a property that many applica-
tion tasks seem to favor. We demonstrate the prevailing ef-
fectiveness of SaBN as a drop-in replacement in four tasks:
conditional image generation, neural architecture search
(NAS), adversarial
training, and arbitrary style trans-
fer. Leveraging SaBN immediately achieves better Incep-
tion Score and FID on CIFAR-10 and ImageNet condi-
tional image generation with three state-of-the-art GANs;
boosts the performance of a state-of-the-art weight-sharing
NAS algorithm significantly on NAS-Bench-201; substan-
tially improves the robust and standard accuracies for ad-
versarial defense; and produces superior arbitrary styl-
ized results. We also provide visualizations and anal-
ysis to help understand why SaBN works. Codes are
available at: https://github.com/VITA-Group/
Sandwich-Batch-Normalization.

SBEVNet: End-to-End Deep Stereo Layout Estimation
Accurate layout estimation is crucial for planning and
navigation in robotics applications, such as self-driving. In
this paper, we introduce the Stereo Bird’s Eye View Network
(SBEVNet) 1, a novel supervised end-to-end framework for
estimation of bird’s eye view layout from a pair of stereo
images. Although our network reuses some of the building
blocks from the state-of-the-art deep learning networks for
disparity estimation, we show that explicit depth estimation
is neither sufﬁcient nor necessary. Instead, the learning of a
good internal bird’s eye view feature representation is effec-
tive for layout estimation. Speciﬁcally, we ﬁrst generate a
disparity feature volume using the features of the stereo im-
ages and then project it to the bird’s eye view coordinates.
This gives us coarse-grained information about the scene
structure. We also apply inverse perspective mapping (IPM)
to map the input images and their features to the bird’s eye
view. This gives us ﬁne-grained texture information. Con-
catenating IPM features with the projected feature volume
creates a rich bird’s eye view representation which is useful
for spatial reasoning. We use this representation to estimate
the BEV semantic map. Additionally, we show that using the
IPM features as a supervisory signal for stereo features can
give an improvement in performance. We demonstrate our
approach on two datasets: the KITTI [5] dataset and a syn-
thetically generated dataset from the CARLA [4] simulator.
For both of these datasets, we establish state-of-the-art per-
formance compared to baseline techniques.

SeeTek: Very Large-Scale Open-set Logo Recognition with Text-Aware Metric
Recent advances in deep learning and computer vision
have set new state of the art in logo recognition [2, 9, 36].
Logo recognition has mostly been approached as a closed-
set object recognition problem and more recently as an
open-set retrieval problem. Current approaches suffer from
distinguishing visually similar logos, especially in open-set
retrieval for very large-scale applications with thousands
of brands. To address the problem, we propose a multi-
task learning architecture of deep metric learning and scene
text recognition. We use brand names as weak labels and
enforce the model to simultaneously extract distinct visual
features as well as predict brand name text. To achieve it,
we collected a dataset with 3 Million logos cropped from
Amazon Product Catalog images across nearly 8K brands,
named PL8K. Our experiments show that adding the task of
text recognition during training boosts the model’s retrieval
performance both on our PL8K dataset and on five other
public logo datasets.

SEGA: Semantic Guided Attention on Visual Prototype for Few-Shot Learning
Teaching machines to recognize a new category based
on few training samples especially only one remains chal-
lenging owing to the incomprehensive understanding of the
novel category caused by the lack of data. However, hu-
man can learn new classes quickly even given few samples
since human can tell what discriminative features should
be focused on about each category based on both the vi-
sual and semantic prior knowledge. To better utilize those
prior knowledge, we propose the SEmantic Guided Atten-
tion (SEGA) mechanism where the semantic knowledge is
used to guide the visual perception in a top-down manner
about what visual features should be paid attention to when
distinguishing a category from the others. As a result, the
embedding of the novel class even with few samples can
be more discriminative. Concretely, a feature extractor is
trained to embed few images of each novel class into a
visual prototype with the help of transferring visual prior
knowledge from base classes. Then we learn a network
that maps semantic knowledge to category-specific atten-
tion vectors which will be used to perform feature selection
to enhance the visual prototypes. Extensive experiments on
miniImageNet, tieredImageNet, CIFAR-FS, and CUB indi-
cate that our semantic guided attention realizes anticipated
function and outperforms state-of-the-art results.

Semantically Stealthy Adversarial Attacks against Segmentation Models
Segmentation models have been found to be vulnerable
to targeted and non-targeted adversarial attacks. However,
the resulting segmentation outputs are often so damaged
that it is easy to spot an attack.
In this paper, we pro-
pose semantically stealthy adversarial attacks which can
manipulate targeted labels while preserving non-targeted
labels at the same time. One challenge is making seman-
tically meaningful manipulations across datasets and mod-
els. Another challenge is avoiding damaging non-targeted
labels. To solve these challenges, we consider each input
image as prior knowledge to generate perturbations. We
also design a special regularizer to help extract features.
To evaluate our model’s performance, we design three ba-
sic attack types, namely ‘vanishing into the context,’ ‘em-
bedding fake labels,’ and ‘displacing target objects.’ Our
experiments show that our stealthy adversarial model can
attack segmentation models with a relatively high success
rate on Cityscapes, Mapillary, and BDD100K. Our frame-
work shows good empirical generalization across datasets
and models.

Sharing Decoders: Network Fission for Multi-task Pixel Prediction
We examine the beneﬁts of splitting encoder-decoders for
multitask learning and showcase results on three tasks (se-
mantics, surface normals, and depth) while adding very few
FLOPS per task. Current hard parameter sharing meth-
ods for multi-task pixel-wise labeling use one shared en-
coder with separate decoders for each task. We generalize
this notion and term the splitting of encoder-decoder archi-
tectures at different points as ﬁssion. Our ablation stud-
ies on ﬁssion show that sharing most of the decoder lay-
ers in multi-task encoder-decoder networks results in im-
provement while adding far fewer parameters per task. Our
proposed method trains faster, uses less memory, results in
better accuracy, and uses signiﬁcantly fewer ﬂoating point
operations (FLOPS) than conventional multi-task methods,
with additional tasks only requiring 0.017% more FLOPS
than the single-task network. We show results with a real-
time model on a Pixel phone with released source code.

SIDE: Center-based Stereo 3D Detector with Structure-aware
Left Image

SpectraNet: Learned Recognition of Artificial Satellites from High Contrast
Effective space domain awareness requires positive iden-
tification of artificial satellites. Current methods for ex-
tracting object identification from observed data require
spatially resolved imagery which limits identification to
objects in low earth orbits. Many artificial Earth satel-
lites, however, operate in geostationary orbits at distances
which prohibit ground based observatories from resolving
spatial information. This paper demonstrates an object
identification solution leveraging modified residual convo-
lutional neural networks to map distance-invariant spec-
troscopic data to object identity. We report classification
accuracies exceeding 80% for a simulated 64-class satel-
lite problem−even in the case of satellites undergoing con-
stant, random re-orientation. An astronomical observing
campaign driven by these results returned accuracies of
∼72% for a nine-class problem with an average of 100 ex-
amples per class, performing as expected from simulation.
We demonstrate the application of variational Bayesian
inference by dropout, stochastic weight averaging (SWA),
and SWA-focused deep ensembling to measure classifica-
tion uncertainties−critical components in space domain
awareness where routine decisions risk expensive space as-
sets and carry geopolitical consequences.

Video-Based Ski Jump Style Scoring from Pose Trajectory
Ski jumping is one of the oldest winter sports and takes
also part in the Winter Olympics from the very start in 1924.
One of the components of the final score, which is used for
ranking the competitors, is the style score, given by five
judges. The goal of this work was to develop a prototype
for automatic style scoring from videos. As the main source
of information, the proposed approach uses the detected lo-
cations of the ski jumper body parts and his skis to capture
a full-body movement through the entire ski jump. We ex-
tended a method for human pose estimation from images to
detect also the tips and the tails of the skies and adapted it to
the domain of ski jumping. We proposed a method to utilize
the detected trajectories along with the scores given by real
judges to build a model for predicting the style scores. The
experimental results obtained on the data that we had avail-
able show that the proposed computer-vision-based system
for automatic style scoring achieves an error comparable to
the error of real judges.

StyleMC: Multi-Channel Based Fast Text-Guided Image Generation
Discovering meaningful directions in the latent space
of GANs to manipulate semantic attributes typically re-
quires large amounts of labeled data. Recent work aims
to overcome this limitation by leveraging the power
of Contrastive Language-Image Pre-training (CLIP), a
joint text-image model. While promising, these meth-
ods require several hours of preprocessing or training
to achieve the desired manipulations. In this paper, we
present StyleMC, a fast and efficient method for text-
driven image generation and manipulation. StyleMC

Tensor-Based Non-Rigid Structure from Motion
In this work we present a method that combines tensor-
based face modelling and analysis and non-rigid structure-
from-motion (NRSFM). The core idea is to see that the con-
ventional tensor formulation for the face structure and ex-
pression analysis can be utilised while the structure com-
ponent can be directly analysed as the non-rigid structure-
from-motion problem. To the NRSFM problem part we fur-
ther present a novel prior-free approach that factorises the
2D input shapes into affine projection matrices, rank-one
3D affine basis shapes, and the basis shape coefficients. The
linear combination of the basis shapes thus yields the recov-
ered 3D shapes upto an affine transformation. In contrast
to most works in literature, no calibration information of
the cameras or structure prior is required. Experiments on
challenging face datasets show that our method, with and
without the metric upgrade, is accurate and fast when com-
pared to the state-of-the-art and is well suitable for dense
reconstruction and face editing.

The Untapped Potential of Off-the-Shelf Convolutional Neural Networks
Over recent years, a myriad of novel convolutional net-
work architectures have been developed to advance state-
of-the-art performance on challenging recognition tasks.
As computational resources improve, a great deal of ef-
fort has been placed on efﬁciently scaling up existing de-
signs and generating new architectures with Neural Archi-
tecture Search (NAS) algorithms. While network topology
has proven to be a critical factor for model performance,
we show that signiﬁcant gains are being left on the table by
keeping topology static at inference-time. Due to challenges
such as scale variation, we should not expect static models
conﬁgured to perform well across a training dataset to be
optimally conﬁgured to handle all test data. In this work,
we expose the exciting potential of inference-time-dynamic
models. We show that by allowing just four layers to dynam-
ically change conﬁguration at inference-time, off-the-shelf
models like ResNet-50 have an upper bound accuracy of
over 95% on ImageNet. This level of performance currently
exceeds that of models with over 20x more parameters and
signiﬁcantly more complex training procedures. While this
upper bound of performance may be practically difﬁcult to
achieve for a real dynamic model, it indicates a signiﬁcant
source of untapped potential for current models.

Towards a Robust Differentiable Architecture Search under Label Noise
Neural Architecture Search (NAS) is the game changer
in designing robust neural architectures. Architectures de-
signed by NAS outperform or compete with the best manual
network designs in terms of accuracy, size, memory footprint
and FLOPs. That said, previous studies focus on developing
NAS algorithms for clean high quality data, a restrictive and
somewhat unrealistic assumption. In this paper, focusing on
the differentiable NAS algorithms, we show that vanilla NAS
algorithms suffer from a performance loss if class labels are
noisy. To combat this issue, we make use of the principle
of information bottleneck as a regularizer. This leads us to
develop a noise injecting operation that is included during
the learning process, preventing the network from learning
from noisy samples. Our empirical evaluations show that the
noise injecting operation does not degrade the performance
of the NAS algorithm if the data is indeed clean. In contrast,
if the data is noisy, the architecture learned by our algorithm
comfortably outperforms algorithms specifically equipped
with sophisticated mechanisms to learn in the presence of
label noise. In contrast to many algorithms designed to
work in the presence of noisy labels, prior knowledge about
the properties of the noise and its characteristics are not
required for our algorithm.

Towards Durability Estimation of Bioprosthetic Heart Valves Via Motion
This paper addresses bioprosthetic heart valve (BHV)
durability estimation via computer vision (CV)-based anal-
yses of the visual symmetry of valve leaflet motion. BHVs
are routinely implanted in patients suffering from valvu-
lar heart diseases. Valve designs are rigorously tested us-
ing cardiovascular equipment, but once implanted, more
than 50% of BHVs encounter a structural failure within
15 years. We investigate the correlation between the visual
dynamic symmetry of BHV leaflets and the functional sym-
metry of the valves. We hypothesize that an asymmetry in
the valve leaflet motion will generate an asymmetry in the
flow patterns, resulting in added local stress and forces on
some of the leaflets, which can accelerate the failure of the
valve. We propose two different pair-wise leaflet symme-
try scores based on the diagonals of orthogonal projection
matrices (DOPM) and on dynamic time warping (DTW),
computed from videos recorded during pulsatile flow tests.
We compare the symmetry score profiles with those of fluid
dynamic parameters (velocity and vorticity values) at the
leaflet borders, obtained from valve-specific numerical sim-
ulations. Experiments on four cases that include three dif-
ferent tricuspid BHVs yielded promising results, with the
DTW scores showing a good coherence with respect to the
simulations. With a link between visual and functional sym-
metries established, this approach paves the way towards
BHV durability estimation using CV techniques.

Transductive Weakly-Supervised Player Detection using Soccer Broadcast
Player detection lays the foundation for many applica-
tions in the ﬁeld of sports analytics including player recog-
nition, player tracking, and activity detection. In this work,
we study player detection in continuous long shot broad-
cast videos. Broadcast match videos are easy to obtain, and
detection on these videos is much more challenging. We
propose a transductive approach for player detection that
treats it as a domain adaptation problem. We show that
instance-level domain labels are signiﬁcant for sufﬁcient
adaptation in the case of soccer broadcast videos. An ef-
ﬁcient multi-model greedy labelling scheme based on visual
features is proposed to annotate domain labels on bounding
box predictions made by our inductive model. We use reli-
able instances from the inductive model inferences to train
a transductive copy of the model. We create and release a
fully annotated player detection dataset comprising soccer
broadcast videos from the FIFA 2018 World Cup matches
to evaluate our method. Our method shows signiﬁcant im-
provements in player detection to the baseline and existing
state-of-the-art methods on our dataset. We show, on aver-
age, a 16 point improvement in mAP for soccer broadcast
videos by annotating domain labels for around a 100 sam-
ples per video.

Transfer Learning for Pose Estimation of Illustrated Characters
Human pose information is a critical component in many
downstream image processing tasks, such as activity recog-
nition and motion tracking. Likewise, a pose estimator for
the illustrated character domain would provide a valuable
prior for assistive content creation tasks, such as refer-
ence pose retrieval and automatic character animation. But
while modern data-driven techniques have substantially im-
proved pose estimation performance on natural images, lit-
tle work has been done for illustrations. In our work, we
bridge this domain gap by efﬁciently transfer-learning from
both domain-speciﬁc and task-speciﬁc source models. Ad-
ditionally, we upgrade and expand an existing illustrated
pose estimation dataset, and introduce two new datasets for
classiﬁcation and segmentation subtasks. We then apply the
resultant state-of-the-art character pose estimator to solve
the novel task of pose-guided illustration retrieval. All data,
models, and code will be made publicly available.

Towards Camera Enabled Touch Typing on Flat Surfaces through
Text entry for mobile devices nowadays is an equally cru-
cial and time-consuming task, with no practical solution
available for natural typing speeds without extra hardware.
In this paper, we introduce a real-time method that is a sig-
nificant step towards enabling touch typing on arbitrary flat
surfaces (e.g., tables). The method employs only a simple
video camera, placed in front of the user on the flat sur-
face — at an angle practical for mobile usage. To achieve
this, we adopt a classification framework, based on the ob-
servation that, in touch typing, similar hand configurations
imply the same typed character across users. Importantly,
this approach allows the convenience of un-calibrated typ-
ing, where the hand positions, with respect to the camera
and each other, are not dictated.
To improve accuracy, we propose a Language Processing
scheme, which corrects the typed text and is specifically de-
signed for real-time performance and integration with the
vision-based signal. To enable feasible data collection and
training, we propose a self-refinement approach that al-
lows training on unlabeled flat-surface-typing footage; A
network trained on (labeled) keyboard footage labels flat-
surface videos using dynamic time warping, and is trained
on them, in an Expectation Maximization (EM) manner.
Using these techniques, we introduce the TypingHands26
Dataset, comprising videos of 26 different users typing on
a keyboard, and 10 users typing on a flat surface, labeled
at the frame level. We validate our approach and present
a single camera-based system with character-level accu-
racy of 93.5% on average for known users, and 85.7% for
unknown ones, outperforming pose-estimation-based meth-
ods by a large margin, despite performing at natural typing
speeds of up to 80 Words Per Minute. Our method is the
first to rely on a simple camera alone, and runs in interac-
tive speeds, while still maintaining accuracy comparable to
systems employing non-commodity equipment.

Unsupervised Learning for Human Sensing Using Radio Signals
There is a growing literature demonstrating the feasibility
of using Radio Frequency (RF) signals to enable key com-
puter vision tasks in the presence of occlusions and poor
lighting. It leverages that RF signals traverse walls and
occlusions to deliver through-wall pose estimation, action
recognition, scene captioning, and human re-identiﬁcation.
However, unlike RGB datasets which can be labeled by hu-
man workers, labeling RF signals is a daunting task because
such signals are not human interpretable. Yet, it is fairly
easy to collect unlabelled RF signals. It would be highly
beneﬁcial to use such unlabeled RF data to learn useful rep-
resentations in an unsupervised manner. Thus, in this paper,
we explore the feasibility of adapting RGB-based unsuper-
vised representation learning to RF signals. We show that
while contrastive learning has emerged as the main tech-
nique for unsupervised representation learning from images
and videos, such methods produce poor performance when
applied to sensing humans using RF signals. In contrast,
predictive unsupervised learning methods learn high-quality
representations that can be used for multiple downstream
RF-based sensing tasks. Our empirical results show that
this approach outperforms state-of-the-art RF-based human
sensing on various tasks, opening the possibility of unsu-
pervised representation learning from this novel modality.

Unsupervised Sounding Object Localization with
Learning to localize sounding objects in visual scenes
without manual annotations has drawn increasing attention
recently. In this paper, we propose an unsupervised sound-
ing object localization algorithm by using bottom-up and
top-down attention in visual scenes. The bottom-up atten-
tion module generates an objectness confidence map, while
the top-down attention draws the similarity between sound
and visual regions. Moreover, we propose a bottom-up at-
tention loss function, which models the correlation relation-
ship between bottom-up and top-down attention. Extensive
experimental results demonstrate that our proposed unsu-
pervised method significantly advances the state-of-the-art
unsupervised methods. The source code is available at
https://github.com/VISION-SJTU/USOL.

Weakly Supervised Learning for Joint Image Denoising and Protein Localization
Deep learning-based object detection methods have
shown promising results in various fields ranging from au-
tonomous driving to video surveillance where input images
have relatively high signal-to-noise ratios (SNR). On low
SNR images such as biological electron microscopy (EM)
data, however, the performance of these algorithms is sig-
nificantly lower. Moreover, biological data typically lacks
standardized annotations further complicating the training
of detection algorithms. Accurate identification of proteins
from EM images is a critical task, as the detected posi-
tions serve as inputs for the downstream 3D structure de-
termination process. To overcome the low SNR and lack of
image annotations, we propose a joint weakly-supervised
learning framework that performs image denoising while
detecting objects of interest. By leveraging per-pixel soft
segmentation and consistency regularization, our frame-
work denoises images without the need of clean images and
is able to detect particles of interest even when less than
0.5% of the data are labeled. We validate our approach on
real single-particle cryo-EM and cryo-electron tomography
(ET) images which are known to suffer from extremely low
SNR, and show that our strategy outperforms existing state-
of-the-art (SofA) methods used in the cryo-EM field by a
significant margin. We also evaluate the performance of our
algorithm under decreasing SNR conditions and show that
our method is more robust to noise than competing methods.

X-MIR: EXplainable Medical Image Retrieval
Despite significant progress in the past few years, ma-
chine learning systems are still often viewed as “black
boxes,” which lack the ability to explain their output de-
cisions. In high-stakes situations such as healthcare, there
is a need for explainable AI (XAI) tools that can help open
up this black box. In contrast to approaches which largely
tackle classification problems in the medical imaging do-
main, we address the less-studied problem of explainable
image retrieval. We test our approach on a COVID-19 chest
X-ray dataset and the ISIC 2017 skin lesion dataset, show-
ing that saliency maps help reveal the image features used
by models to determine image similarity. We evaluated three
different saliency algorithms, which were either occlusion-
based, attention-based, or relied on a form of activation
mapping. We also develop quantitative evaluation metrics
that allow us to go beyond simple qualitative comparisons
of the different saliency algorithms. Our results have the
potential to aid clinicians when viewing medical images
and addresses an urgent need for interventional tools in re-
sponse to COVID-19. The source code is publicly available
at: https://gitlab.kitware.com/brianhhu/x-mir.

Small or Far Away? Exploiting Deep Super-Resolution and Altitude Data for
Visuals captured by high-flying aerial drones are in-
creasingly used to assess biodiversity and animal popula-
tion dynamics around the globe. Yet, challenging acquisi-
tion scenarios and tiny animal depictions in airborne im-
agery, despite ultra-high resolution cameras, have so far
been limiting factors for applying computer vision detec-
tors successfully with high confidence.
In this paper, we
address the problem for the first time by combining deep
object detectors with super-resolution techniques and alti-
tude data. In particular, we show that the integration of a
holistic attention network based super-resolution approach
and a custom-built altitude data exploitation network into
standard recognition pipelines can considerably increase
the detection efficacy in real-world settings. We evalu-
ate the system on two public, large aerial-capture animal
datasets, SAVMAP and AED. We find that the proposed ap-
proach can consistently improve over ablated baselines and
the state-of-the-art performance for both datasets. In ad-
dition, we provide a systematic analysis of the relationship
between animal resolution and detection performance. We
conclude that super-resolution and altitude knowledge ex-
ploitation techniques can significantly increase benchmarks
across settings and, thus, should be used routinely when de-
tecting minutely resolved animals in aerial imagery.
1. Introduction

Supervised Contrastive Learning for Generalizable and Explainable DeepFakes
DeepFakes detection approaches have to be agnostic
across generation type, quality, and appearance to provide
a generalizable DeepFakes detector. Limited generalizabil-
ity will hinder wide-scale deployment of detectors if they
cannot handle unseen attacks in an open set scenario. We
propose a generalizable detection model that can detect
novel and unknown/unseen DeepFakes using a supervised
contrastive (SupCon) loss. As DeepFakes can resemble the
original image/video to a greater extent in terms of appear-
ance and it becomes challenging to secern them, we propose
to exploit the contrasts in the representation space to learn
a generalizable detector. We further investigate the features
learnt from our proposed approach for explainability. The
analysis for explainability of the models advocates the need
for fusion and motivated by this, we fuse the scores from
the proposed SupCon model and the Xception network to
exploit the variability from different architectures. The pro-
posed model consistently performs better compared to the
single model on both known data and unknown attacks con-
sistently in a seen data setting and an unseen data setting,
with generalizability and explainability as a basis. We ob-
tain the highest accuracy of 78.74% using proposed SupCon
model and an accuracy of 83.99% with proposed fusion in
a true open-set evaluation scenario where the test class is
unknown at the training phase. The paper also aligns with
reproducible research by making the code available 1.

Feature-Align Network with Knowledge Distillation for Efficient Denoising
We propose an efficient neural network for RAW image
denoising. Although neural network-based denoising has
been extensively studied for image restoration, little atten-
tion has been given to efficient denoising for compute lim-
ited and power sensitive devices, such as smartphones and
wearables. In this paper, we present a novel architecture
and a suite of training techniques for high quality denois-
ing in mobile devices. Our work is distinguished by three
main contributions. (1) The Feature-Align layer that mod-
ulates the activations of an encoder-decoder architecture
with the input noisy images. The auto modulation layer
enforces attention to spatially varying noise that tends to
be ”washed away” by successive application of convolu-
tions and non-linearity. (2) A novel Feature Matching Loss
that allows knowledge distillation from large denoising net-
works in the form of a perceptual content loss. (3) Empir-
ical analysis of our efficient model trained to specialize on
different noise subranges. This opens an additional avenue
for model size reduction by sacrificing memory for compute.
Extensive experimental validation shows that our efficient
model produces high quality denoising results that com-
pete with state-of-the-art large networks, while using sig-
nificantly fewer parameters and MACs. On the Darmstadt
Noise Dataset benchmark, we achieve a PSNR of 48.28dB,
while using 263× fewer MACs and 17.6× fewer parameters
than the state-of-the-art network, which achieves 49.12dB.

Argus++: Robust Real-time Activity Detection
Activity detection is one of the attractive computer vi-
sion tasks to exploit the video streams captured by widely
installed cameras. Although achieving impressive per-
formance, conventional activity detection algorithms are
usually designed under certain constraints, such as using
trimmed and/or object-centered video clips as inputs. There-
fore, they failed to deal with the multi-scale multi-instance
cases in real-world unconstrained video streams, which are
untrimmed and have large field-of-views. Real-time require-
ments for streaming analysis also mark brute force expansion
of them unfeasible.

Subjective Quality Assessment of User-Generated Content Gaming Videos
Benefited from the rapid development of the digital game
industry, the growing popularity of online user-generated
content (UGC) videos for games has accelerated the de-
velopment of perceptual video quality assessment (VQA)
models specifically for gaming videos. As a novel UGC
type, gaming videos are recorded by gamers and uploaded
to major streaming media platforms such as YouTube and
Twitch, and have been extremely popular among the audi-
ence. However, there is little work on VQA research re-
lated to gaming videos and understanding their character-
istics.
In order to promote the development of the gam-
ing VQA model, we created a new UGC gaming video
VQA resource, named LIVE-YouTube Gaming video qual-
ity (LIVE-YT-Gaming) database, composed of 600 authen-
tic UGC gaming videos and 18,600 subjective quality rat-
ings collected from an online subjective study. We also
compared and analyzed several state-of-the-art (SOTA)
VQA models on the new database. To support work in
this field,
the new database will be publicly available
through the link: https://live.ece.utexas.edu/
research/LIVE-YT-Gaming/index.html.

